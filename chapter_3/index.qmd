# A principled workflow for secure and replicable data management

There are three key issues that we have to grapple with when it comes to data management. The first is that we need to ensure that our data are secure for the protection of our subjects. The second is that we need to ensure that our original data remain intact and any changes need to be documented for the purposes of replicability. The final concern we should consider is that we should have quick access to our data, even when contained within relatively large datasets. This chapter presents a principled workflow that should aid researchers achieve these three goals.

In the United States, Institutional Review Boards (IRBs) are tasked with the responsibility of enforcing federal regulations that require that researchers keep their data secure and that they protect the confidentiality of research subjects. This means that we have an obligation to ensure that no one can discover the identity of a participant either through obtaining information that identifies a participant or through inferring the identity of a participant through a combination of the data that we collect and share. While enforcement of these regulations can vary to some extent, this often means that we need to ensure that our data stays stored on a secure computer or locked away if they are physical files and that we "de-identify" our data. While this is relatively easy to do and protects the confidentiality of our research subjects, this butts up against increasing calls that we make our research replicable at every step of the process which often means that we need to share de-identified data with journals upon publication of our papers.

Resulting from a significant number of high-profile cases of famous academic papers and researchers who have been accused of mis-handling their data, there are increasingly higher expectations for researchers to provide documentation and materials that allow researchers to independently re-do your study. This occurs as the social sciences currently grapple with a "Replication Crisis" where there are many papers being retracted for a number of issues with the empirical evidence presented in them.

While we need to be protective of our data and at the same time willing to share it so that others may access it, we have a third challenge that is increasingly salient for researchers: we need to be able to quickly access our data. This is a basic need for researchers to be able to do their jobs and to not be hung up on spending hours loading data or portions of their data, as well as a need to increase the accessibility of our data when we share it with other researchers that may have varying computational resources available to them.

These tensions creates a very tricky situation for researchers. We are expected to ensure that others cannot identify any of our participants while it is extremely hard to verify the validity of a researchers claims without being able to verify that the authors took the proper and reported steps at each and every stage of the process. One reason this is so tricky is because there is no concrete set of standards or best practices that researchers follow when it comes to data management.

While we receive extensive training and spill a significant amount of ink about where to collect data, what data to collect and how to analyze them, we receive little-to-no guidance in what to do with the data between the steps of collecting and analyzing those data. We follow IRB regulations by storing our data on a password protected computer and then make judgment calls about what subset of our data to share publicly with the journal and readers of our papers. Therefore we have non-standardized processes for documenting the steps to de-identify and clean our data. This naturally leads to very idiosyncratic workflows for data management and reflects the extent to which these steps are an afterthought in the research process. The goal of this chapter is to convince you that the current state of data management in the social sciences is indeed poses a problem and that we do not have to be this way.

## Examples of current data management workflows

Let me provide a couple of case studies of the current way that people manage their data.

**Case Study 1:** You download a `.csv` (or `.xlsx`, `.dta`, or even a `.sav`) file from the site that you hosted your study and collected your data on. You then open the file and start relabeling things. You may have a column that originally comes in something like "VAR_001" and you change the column name to "PARTICIPANT_ID". You also notice that some of the variables have rows with the label of the response rather than an integer. So you start changing any cell that says "Strongly Disagree" to "1". Once you have gone through, this you save a new copy of the file and call it "cleaned-data.csv". Then you load the file and start performing descriptive statistics and fit simple regressions to get a preliminary sense of whether your study paid off. If you find any mistakes, you open the "cleaned-data.csv" file again to fix them or go back to the original file and restart the cleaning process. Or you may even write code to correct the mistakes in the "cleaned-data.csv" file rather than having to deal with fixing those mistakes by restarting or trying to find them in a few-hundred row spreadsheet.

There are a number of things that are dangerous to this process. The first is that your data management and data pre-processing is not at all replicable. The goal of replicability is that someone else can take your original data, and they can follow step-by-step what you did to come to the exact same conclusions.

Manually editing the original file does not populate any documentation about what things you are clicking or typing. There is so little documentation, that you might not even be able to replicate the steps you took to manage and pre-process your data; let alone someone else. Even scarier, your finger may slip and you type the wrong number or press the delete key while going over the wrong cell. Sometimes you catch this, but sometimes you may not. Without this documentation, if another researcher finds discrepancies there can only be speculation as to whether those discrepancies arose from research design choices or data management issues. Not only can this carry professional costs, but it also limits our ability to be confident that we understand the true answer to your question.

The other problem is that these steps are highly inefficient. Manually going through cells on a spreadsheet may take a few minutes to a few hours to clean if you have a few hundred rows with a dozen or so columns. However, many of the data sets that we use in the social sciences are often approaching thousands (if not millions) of rows and hundreds of columns. To do this for such a large dataset would take a significant amount of time and any mistakes would be extremely hard to catch in such a vast amount of data.

The most likely thing that will happen when asked to share your data with a journal upon publication of your research is that you will just share the cleaned excel file. Why? Well, because you are not going to want to reinvent the wheel. At this step, it can also be quite dangerous. Not only does this cause the lack of replicability of your analysis, but you may forget to remove identifiable information from the file. If this happens, you have failed to meet federal regulations requiring that you take every effort to ensure your participants are not easily identifiable.

**Case Study 2:** Similar to Case Study 1, you download your file. Instead, you open up an R, STATA, or Python session. You start writing code in a cleaning script that renames the columns, and recodes columns to have the correct integer values. You then start writing code to perform descriptive statistics and to fit simple regressions for preliminary analyses. While doing this, you may notice some mistakes and so you go back to your code and make some adjustments to erase those mistakes.

This process is a little bit more replicable and efficient than the first one. Writing code allows you to systematically make changes to any cell where some set of conditions are applied. The code also does this relatively quickly (in many cases). This also is safer (in terms of replicability) as the code acts as your documentation as to what you changed from the original dataset that you had. 

While this is a vast improvement over the first case, there are still some aspects of this that are relatively dangerous. The first is that the replicability and accuracy of your documentation is still heavily dependent on whether you write your code as a script or run the code line-by-line (interactively). Running your code as a script means that you run the whole file at once rather than running it line-by-line (interactively). Executing this code as a script is much more replicable because it does not allow you to cherry-pick which lines of code you execute, but it requires your code to be clean and to only execute what you intend to execute. The second is that there is no clear step to ensure that your data is de-identified to ensure that you do not accidentally share data that would betray the identity of your subjects. Even still, if you take steps to de-identify the data, you cannot include that code in your script or else it will throw errors back at an independent researcher that only has the de-identified version of the data. So this still means that there are a high number of researcher degrees of freedom by which some researchers can be more or less compliant with protecting the anonymity of their subjects as well as providing documentation for replicating one's analyses.

In the rest of this chapter I will introduce and advocate for a workflow that should enable replicable, efficient, and secure files for one's research. As I will argue, this process removes much of the ad-hoc and idiosyncratic processes by replacing particular choices with a formulaic and principled set of steps for one to follow when processing data from a study. What this process also incentivize is that researchers use specialized tools designed to be used for the distinct steps of the process. As the next chapter will grapple with more, moving away from using a single tool for all steps of the research process encourages one to follow the best practices for each step as these tools' capabilities are a reflection of the widely-accepted best practices for that particular task.

## A principled data management workflow

@fig-workflow provides an overview to my proposed workflow to data management. The remainder of this chapter will cover each of these steps and explain what the value added is in terms of security, replicability, and efficiency by using this workflow.

{{< include ../assets/_workflow.qmd >}}

In **Step 1**, you should make sure to download a CSV file if possible. Why a CSV file specifically? Other file types such as STATA's `.dta`, Microsoft Excel's `.xlsx` and other common ones often have some degree of proprietary protection of them and one's ability to access that raw data is dependent on these companies' continued willingness to allow for users to develop packages that allow for one to load that file. 

Further, as I will advocate for in the next chapter, one should use the `SQL` language for their data management as it is designed for such a task and therefore provides incentives to follow best practices. DuckDB's implementation of `PostgreSQL` enables one to create tables from a raw data `CSV` file and to initiate a new `.duckdb` file without having to open the original data file or loading it with some other language. This reduces the chances that one makes changes to the data that are not documented either through automatic formatting that sometimes occurs by Microsoft Excel, Apple Numbers, or some other software when parsing a `CSV` file to make it more readable in a tabular format as well as reducing the temptation to perform ad-hoc data management by loading the file into R, Python, Julia or some other language for data analysis.

Once you have downloaded the raw data as a `CSV` file, in you can immediately load and interface with it using `DuckDB`. Once you have done this, in **Step 2** you should create some sort of case (participant) identifier column. This can be as simple as creating a column that records the current row number for the participant. As you will see based on my recommendations in the next few steps, the goal of this is that you should decentralize your data so that you ensure that you do not accidentally share information that makes it easy for people to identify your participants, while also not deleting data so that internally you may have access the original data in its complete form.

In **Step 3** you can start creating and storing tables in your `.duckdb` file. You should pull any identifiable data from the loaded data and store it in a separate table. This sort of information would certainly be columns containing the names and addresses (mailing or IP), Session ID's, Time and/or location they took the study from, as well as any Participant ID provided by a vendor. With this subset of your original data, you can store a table with a copy your Participant ID column (from **Step 2**). 

Keeping these data allow you to immediately de-identify your data while also not deleting it so that you are able to continue to use any of that data in case you need to apply exclusion restrictions, use those data for payment to participants, confirm participation in a study, etcetera. The generated participant id column allows you to merge data from that table with identifiable information if need be at a later time, but ensures that you do not have *any* data that may make it easy to identify your participants in an analysis or any of your files that you make publicly available.

Taking this step early on not only helps with ensuring the confidentiality of your participants, but doing it this way helps with replicability in that you will have to write `SQL` code that documents every step you took from downloading the file on your computer through your analysis. It also aids in efficiency in that you will still have those data that you can easily merge with the main subset of your data stored in the second table in case you need to when figuring out which participants to exclude from the study, confirm whether participants completed the study and are eligible for compensation, etcetera. Also, if internally, you need to demonstrate that your data is intact, then you can easily merge on your generated participant id column.

Once you have pulled out the identifiable information about your subjects that you want to keep separate from the subset you will use for your analyses, you will want to store both as separate tables in your `.duckdb` file. So, by the end of **Step 4**, you should have a `.duckdb` file saved on your computer that contains two tables: a table with your participant id column (**Step 2**) that contains identifiable information about your participants, and a second table that contains the same participant id column (**Step 2**) with the data that you will use for your analyses.

The benefit of having a `.duckdb` file rather than separate files or a single excel file with multiple sheets containing the same amount of information is that if there is unauthorized access to your computer a `.duckdb` file requires that someone write `SQL` code in order to view any of the contents of the file. This increases security in the event that someone gains access to the file. Obviously if the person with unauthorized access to the file understands `SQL` they can access the data. However, the architecture of the file (having multiple tables and knowing which column contains what information and what that information represents) also requires knowledge about the file that should be ideally stored as some sort of codebook or separate documentation. The increased requirements for technical know-how and of internal documentation of the "schema" (the structure of the tables within the file) significantly increases the complication of viewing the data for those that are not part of the research team, thus increasing security and your ability to retain the confidentiality of your subjects' identities.

Once we have completed **Step 4**, we can begin to write `SQL` code to clean our data in **Step 5**. While this asks researchers to eschew packages and languages that many academics have become accustomed to using, as I argue in the next chapter, it ensures that we are using the right tool for the task. `SQL` is a language designed with the express purpose of data management. Because of its specialization, this means that the language's capabilities, design, and workflow are optimized for this particular task. There is also something to be said about the psychological benefits of using different tools for different steps of the research process -- it encourages you to consider each step as complete and separate, therefore encouraging better documentation. 

As I will discuss more in the following chapter as well, many of the languages that researchers use for data analysis (and as a consequence default to using the same language for data management), have very large and very popular packages for data management. While these packages are large, open source, and have teams of professional developers who consistently seek to increase the functionality and efficiency of the functions in those packages, they also change rapidly by changing names of functions, implementing new functions, depreciating functions, and make adjustments to how those functions work. These characteristics are the main sources of criticism towards using them as they create many headaches for researchers as new versions of these packages often create new errors or depreciation warnings that make it harder to be confident in the continued replicability of one's code. There are also other criticisms to some of these packages as they are very reliant on other functions (have a relatively higher number of dependencies) and if those packages that they depend upon change or are no longer supported, this creates problems as well as it can limit how fast one's code runs as well as the replicability of one's code.

These features are reflective of what these languages are often built for: data analysis. As we innovate in the models and techniques available to us to detect patterns in our data, the need for constant integration of these new tools encourages developers and applied users to adjust our code to reflect these changes in standards and techniques.

`SQL`, however, is a quite old language (in terms of coding languages existing today). It also has not changed much. There are two primary flavors of `SQL` such as the open-source `PostgreSQL` that `duckdb` relies upon. Given that most needed innovations in data management are focused on a user's ability to access an increasingly larger volume of data in a shorter amount of time, these demands do not require or incentivize changes to the language's functions that a researcher would use. As a result, we do not need to worry about our `SQL` code needing to be updated in response to changes to function names or any depreciations. The only thing that really changes are the versions of the packaged software that we interface with, but the underlying code does not because the developers are responding to demands to make the database file sizes smaller, for them to load faster, and to do so with ballooning datasets; but the common need to select certain columns, rename them, use row-wise and column-wise aggregations, etc. do not change.

If I have successfully convinced you to do the data cleaning with `SQL` rather than using a package in a language for data analysis, we will want to save a *third* table that contains the cleaned data you will use in your analyses. From there, in **Step 6** we can begin to perform our analyses by loading this third, cleaned, table of the data for our analyses.

After we have completed our analyses and we are preparing our paper for submission to a journal or to share it publicly, we will want to perform **Step 7** which means that we will create and save a new `.duckdb` file. This second `.duckdb` file will store a copy of the cleaned and de-identified data table(s) we use in our analyses as well as the table containing the original and de-identified data.
