# Introduction

There have been a significant number of high-profile cases of famous academic papers and researchers who have been accused of mis-handling their data. This occurs as the social sciences currently grapple with a "Replication Crisis" where there are many papers being retracted for a number of issues with the empirical evidence presented in them. While we may naturally default to thinking of these as instances of fraud or at least extreme amounts of incompetence, I see this as a systematic issue. While we receive extensive training and spill a significant amount of ink about where to collect data, what data to collect and how to analyze them, we receive little-to-no guidance in what to do with the data between the steps of collecting and analyzing those data. This naturally leads to very idiosyncratic workflows for data management and processing and reflects the extent to which these steps are an afterthought in the research process. The goal of this chapter is to convince you that the current state of data management and pre-processing in the social sciences is indeed poses a problem and that we do not have to be this way. 

Let me provide a couple of case studies of the current way that people manage their data.

<!--
Note:

- In the following bits discussing these case studies use the first para. to discuss these processes, then in a second dissect what potential problems there are with them.

-->
**Case Study 1:** You download a `.csv` (or `.xlsx`, `.dta`, or even a `.sav`) file from the site that you hosted your study and collected your data on. You then open the file and start relabeling things. You may have a column that originally comes in something like "VAR_001" and you change the column name to "PARTICIPANT_ID". You also notice that some of the variables have rows with the label of the response rather than an integer. So you start changing any cell that says "Strongly Disagree" to "1". Once you have gone through, this you save a new copy of the file and call it "cleaned-data.csv". Then you load the file and start performing descriptive statistics and fit simple regressions to get a preliminary sense of whether your study paid off. If you find any mistakes, you open the "cleaned-data.csv" file again to fix them or go back to the original file and restart the cleaning process. Or you may even write code to correct the mistakes in the "cleaned-data.csv" file rather than having to deal with fixing those mistakes by restarting or trying to find them in a few-hundred row spreadsheet.

There are a number of things that are dangerous to this process. The first is that your data management and data pre-processing is not at all replicable. The goal of replicability is that someone else can take your original data, and they can follow step-by-step what you did to come to the exact same conclusions. Manually editing the original file does not populate any documentation about what things you are clicking or typing. There is so little documentation, that you might not even be able to replicate the steps you took to manage and pre-process your data; let alone someone else. Even scarier, your finger may slip and you type the wrong number or press the delete key while going over the wrong row. Sometimes you catch this, but sometimes you may not. The other problem is that these steps are highly inefficient. Manually going through cells on a spreadsheet may take a few minutes to a few hours to clean if you have a few hundred rows with a dozen or so columns. However, many of the data sets that we use in the social sciences are often approaching thousands (if not millions) of rows and hundreds of columns. To do this for such a large dataset would take a significant amount of time.

**Case Study 2:** Similar to Case Study 1, you download your file. Instead, you open up an R, STATA, or Python session. You start writing code in a cleaning script that renames the columns, and recodes columns to have the correct integer values. You then start writing code to perform descriptive statistics and to fit simple regressions for preliminary analyses. While doing this, you may notice some mistakes and so you go back to your code and make some adjustments to erase those mistakes.

This process is a little bit replicable and efficient than the first one. Writing code allows you to systematically make changes to any cell where some set of conditions are applied. The code also does this relatively quickly (in many cases). This also is safer (in terms of replicability) as the code acts as your documentation as to what you changed from the original dataset that you had. While this is a vast improvement over the first case, there are still some aspects of this that are relatively dangerous. The first is that when you notice mistakes, you may just jump into the file itself and make some changes or you may add a new line to your code to make this adjustment. The latter of these is less dangerous than the first. But as we will continue to discuss throughout the book, how safe this latter choice is depends heavily on which language your code is in.  

**Case Study 3:** You may have fit in with one of the first two case studies when you originally had collected the data and wanted to analyze it. Perhaps, now you are working with a coauthor or have received feedback on your research after presenting it at a conference or from a reviewer for a journal that you submitted your work to. It has been at least a few weeks (if not months or years) since you have last looked at the original data and had cleaned it. But now you are being forced to. You can't remember exactly what you had done when manually cleaning it in the spreadsheet or whether all lines of your code work and were ran when you initially did this. So now you essentially need to start this process of cleaning your data all over again. So you do it and add in the additional adjustments that someone else wanted. Or perhaps, you are 99% sure that everything is up-to-date and you just need to make the recommended changes from your reviewer or coauthor but do not change anything else.


<!--
Note:

- Once I have laid out these case studies, I should next dig into what my proposed workflow offers and why we should be using tools that are optimal for each step of the process.
-->


The goal of this chapter is set the foundations for implementing a principled data workflow. To achieve this goal, we will leave some of the coding for the next chapter. The point of implementing a principled workflow is to refine the heterogenous and often ad-hoc workflow that many researchers use. The hope is that this will foster a consistent and formulaic approach to 

