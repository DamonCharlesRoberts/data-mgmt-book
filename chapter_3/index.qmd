# A principled workflow for secure and replicable data management

There are three key issues that we have to grapple with when it comes to data management. The first is that we need to ensure that our data are secure for the protection of our subjects. The second is that we need to ensure the replicability of our project by keeping original data intact and any changes need to be documented. The final concern we should consider is that we should have quick access to our data, even when contained within relatively large datasets -- I will refer to this as efficiency. This chapter presents a principled workflow that should aid researchers achieve these three goals.

In the United States, Institutional Review Boards (IRBs) are tasked with the responsibility of enforcing federal regulations that require that researchers keep their data secure and that they protect the confidentiality of research subjects. This means that we have an obligation to ensure that no one can uncover the identity of a participant either through obtaining information that directly identifies a participant or by using a combination of data that we collect to infer a participant's identity. 

While enforcement of these regulations can vary to some extent, this often means that we need to ensure that our data stays stored on a secure computer or locked away if they are physical files and that we "de-identify" our data. Thankfully many computers come with increasingly sophisticated security to the underlying files stored on our hard drive. This often means that if we lose a computer, we are generally safe so long as we have used a password to restrict access to your files or by using more sophisticated credentials such as a security key, biometrics, or by using 2 Factor Authentication. Many of us often use some sort of cloud-based file storage such as Dropbox, Google Drive, or iCloud -- which are all password protected and often mean that many of our files are not physically stored on our computer's hard drive (in the traditional sense). While securing our computer and our files on them is relatively easy to do and protects the confidentiality of our research subjects, maintaining the confidentiality of our subjects has become a bit more difficult due to increasing calls that we make our research replicable at every step of the process which often means that we need to share data with journals upon publication of our papers.

Resulting from a significant number of high-profile cases of famous academic papers and researchers who have been accused of mis-handling their data, there are increasingly higher expectations for researchers to provide documentation and materials that allow researchers to independently re-do your study. This occurs as the social sciences currently grapple with a "Replication Crisis" where there are many papers being retracted for a number of issues with the empirical evidence presented in them.

The common approach that researchers employ is to share the cleaned version of the dataset, that is often de-identified, with journals so that others may independently replicate the study. Some criticize this practice by arguing that this is still not entirely replicable. Many of these famous instances of data mis-handling shared "cleaned" versions of their datasets. This has lead many to argue that our analyses are not fully replicable until we can replicate literally every step of the process -- from taking the raw data, cleaning it, and then analyzing it. This puts researchers who want their work to be replicable in a bind.

We cannot share our data in its rawest form due to our need to protect the identities of our participants. There will necessarily be some changes to our raw data that others cannot see and replicate. We can share raw but de-identified data, however. If we follow a consistent and principled set of practices to managing our data, then we should be able to provide data files that allow others to replicate essentially every step of our research, at least the parts that will impact our analyses.

While we need to be protective of our data and at the same time willing to share it so that others may access it, we have a third challenge that is increasingly salient for researchers: we need to be able to quickly access our data. There are two things that limit efficiency when it comes to data management: 1) protecting the identities of our participants through keeping our files secure and 2) the increasing size of our datasets in the social sciences.

A common paradox in security is that the more secure something is, the more inconvenient it is to use. A common example is internet accounts. 2 Factor Authentication has been a major advancement over passwords to secure our online accounts. However, many bemoan the added work one has to perform to access an account because of this extra step to login to an account. This same paradox can apply to protecting the identities of our research participants.

We can take extra steps besides storing the files behind a required login (either as a login to an account on a computer and/or a login to a cloud service). These steps could be things like encrypting the files and password protecting the files themselves, storing these files on a secure server hosted by our organization or an approved vendor, or not collecting *any* identifiable information whatsoever. However, these extra steps are not taken because they are a lot of extra work and slow down our ability to access the data for analysis as well as complicate the process of sharing our data for replication purposes with other researchers.

The second source of inefficiency for data management is that many of our datasets are becoming much larger (both in terms of number of observations, but almost universally in terms of the number of variables we collect data on). These datasets are becoming larger as computational social science and its techniques are increasing in popularity as well as higher competition for journal space for tests of causal hypotheses that often mean that reviewers and editors are increasingly critical of which confounds or moderators are excluded from some statistical test.

This increase in size of our datasets means that there are increasingly higher requirements for computational resources, resources that we may not always have access to or resources that others replicating our work may not have either. As a result, we need to be particularly concerned about the ease of accessing our data. Thankfully computers are increasingly powerful and come standard with more computational resources. However, these increases in resources are not always show up as large performance gains as many basic applications we rely upon use up more resources due to increased functionality. Additionally, a reliance on the increased availability to more computational resources assumes that we have the latest models of computers, which is not always the case. Relying on advancements in standard offerings of hardware is a relatively fragile position to be in. As a result, we can look for software solutions that have low resource overhead to maximize how much of our available resources may be dedicated to our research.

These three goals are all inter-related and as I have pointed out produce a number of tensions that creates a very tricky situation for researchers. We need to take every effort available to us to protect the identities of our participants while simultaneously attempting to avoid making our files so secure that it takes a significant amount of effort to access them as well as making efforts to assuage concerns of others that we are not engaging in academic malpractice by sharing as much documentation and raw data as possible with others. One reason the current state of data management for academic researchers is so tricky is because there is no concrete set of standards or best practices that researchers follow when it comes to data management.


Looking at posted syllabi and materials for the "Math Camp" that incoming political science PhD students take as well as introductory quantitative research methods classes for the "Top-10" departments (As defined by U.S. News and World Report), I see some departments such as Harvard discuss version control for files as well as how to document your research and code. However, there are no primers or sessions dedicated to managing one's data or how to pre-process it. I think this reflects the experiences of many. We learn how to manage and pre-process our data as we go through these courses and as we work on projects with faculty. This ad-hoc approach to learning these skills also show up in the inconsistency by which researchers manage and pre-process their data. I do not believe it to be a stretch that many of us have quite different experiences working with different collaborators on how the team manages and pre-processes the data for a project. 

While we receive extensive training and spill a significant amount of ink about where to collect data, what data to collect and how to analyze them, my examination of syllabi in top departments for their introductory quantitative research methods courses and bootcamps confirms my suspicions that we receive little-to-no guidance in what to do with the data between the steps of collecting and analyzing those data. Beyond this training, we follow IRB regulations by storing our data on a password protected computer and then make judgment calls about what subset of our data to share publicly with the journal and readers of our papers. Because of the inconsistency in training and enforcement mechanisms, we have non-standardized processes for managing our data. This naturally leads to very idiosyncratic workflows for data management and these patterns reflects the extent to which these steps are an afterthought in the research process. The goal of this chapter is to convince you that the current state of data management in the social sciences indeed poses a problem and that we can be more secure, replicable, and efficient in terms of data management.

## Examples of current data management workflows

Let me provide a couple of case studies of the current way that people manage their data.

**Case Study 1:** You download a `.csv` (or `.xlsx`, `.dta`, or even a `.sav`) file from the site that you hosted your study and collected your data on. You then open the file and start relabeling things. You may have a column that originally comes in something like "VAR_001" and you change the column name to "PARTICIPANT_ID". You also notice that some of the variables have rows with the label of the response rather than an integer. So you start changing any cell that says "Strongly Disagree" to "1". Once you have gone through, this you save a new copy of the file and call it "cleaned-data.csv". Then you load the file and start performing descriptive statistics and fit simple regressions to get a preliminary sense of whether your study paid off. If you find any mistakes, you open the "cleaned-data.csv" file again to fix them or go back to the original file and restart the cleaning process. Or you may even write code to correct the mistakes in the "cleaned-data.csv" file rather than having to deal with fixing those mistakes by restarting or trying to find them in a few-hundred row spreadsheet.

There are a number of things that are dangerous to this process. The first is that your data management and data pre-processing is not at all replicable. The goal of replicability is that someone else can take your original data, and they can follow step-by-step what you did to come to the exact same conclusions.

The first thing that is dangerous is that your file is not secure. If you leave your computer open or logged-in and someone gets access to it, they can quickly and easily open that `.csv` file. Confidentiality for your participants are extremely dependent on your choices to ensure that your computer is not easily accessed by others. While often password protecting our computers is often sufficient to meet the IRB guidelines that social scientists often need to meet, we often store these files on a cloud-based hosting service such as Dropbox, Google Drive, or iCloud. Both ways of storing these files means that they are only as secure as our accounts are. As we often see in the news, only using a password protected account often is not sufficient to keep prying eyes from accessing our account. In other words, there is nothing stopping us from taking a few extra steps to make it harder for those without authorized access to view data that needs to be kept confidential. Unlike common steps like using 2 Factor Authentication that require a lot of extra work only for the purposes of security, the workflow that I advocate for later in this chapter actually makes access to our data more efficient, replicable, and secure -- it finds a balance to achieve all three goals without having to sacrifice one of these goals in the name of security as the common paradox for security often posits.

The second issue relates to the reproducibility of your data management. Manually editing the original file does not populate any documentation about what things you are clicking or typing. There is so little documentation, that you might not even be able to replicate the steps you took to manage and pre-process your data; let alone someone else. Even scarier, your finger may slip and you type the wrong number or press the delete key while going over the wrong cell. Sometimes you catch this, but sometimes you may not. Without this documentation, if another researcher finds discrepancies there can only be speculation as to whether those discrepancies arose from research design choices or data management issues. Not only can this carry professional costs, but it also limits our ability to be confident that we understand the true answer to your question.

The final issue is that these steps are highly inefficient. Manually going through cells on a spreadsheet may take a few minutes to a few hours to clean if you have a few hundred rows with a dozen or so columns. However, many of the data sets that we use in the social sciences are often approaching thousands (if not millions) of rows and hundreds of columns. To do this for such a large dataset would take a significant amount of time and any mistakes would be extremely hard to catch in such a vast amount of data. This also all assumes that the dataset is not so large that common software -- such as Microsoft Excel -- actually opens the file and that we are not filling up precious space on our Hard Drive to store such a large file. 

The most likely thing that will happen when asked to share your data with a journal upon publication of your research is that you will just share the cleaned excel file. Why? Well, because you are not going to want to reinvent the wheel. At this step, it can also be quite dangerous. Not only does this cause the lack of replicability of your analysis, but you may forget to remove identifiable information from the file -- there is no real clear step that you had to take to remove this information, so in the fog of trying to get a paper prepared for publication, it is relatively easy to forget to do this if it is not baked into your process. If this happens, you have failed to meet federal regulations requiring that you take every effort to ensure your participants are not easily identifiable.

**Case Study 2:** Similar to Case Study 1, you download your file. Instead, you open up an R, STATA, or Python session. You start writing code in a cleaning script that renames the columns, and recodes columns to have the correct integer values. You then start writing code to perform descriptive statistics and to fit simple regressions for preliminary analyses. While doing this, you may notice some mistakes and so you go back to your code and make some adjustments to erase those mistakes.

This process is a little bit more replicable and efficient than the first one. Writing code allows you to systematically make changes to any cell where some set of conditions are met. The code also does this relatively quickly (in many cases). This also is safer (in terms of replicability) as the code acts as your documentation as to what you changed from the original dataset that you had. 

While this is a vast improvement over the first case, there are still some aspects of this that are relatively dangerous. The first is that the replicability and accuracy of your documentation is still heavily dependent on whether you write your code as a script or run the code line-by-line (interactively). Running your code as a script means that you run the whole file at once rather than running it line-by-line (interactively). Executing this code as a script is much more replicable because it does not allow you to cherry-pick which lines of code you execute, but it requires your code to be clean and to *only* execute what you intend to execute. The second is that there is no clear step to ensure that your data is de-identified to ensure that you do not accidentally share data that would betray the identity of your subjects. Even still, if you take steps to de-identify the data, you cannot include that code in your script or else it will throw errors back at an independent researcher that only has the de-identified version of the data. So this still means that there are a high number of researcher degrees of freedom by which some researchers can be more or less compliant with protecting the anonymity of their subjects as well as providing documentation for replicating one's analyses.

In the rest of this chapter I will introduce and advocate for a workflow that should enable replicable, efficient, and secure files for one's research. As I will argue, this process removes much of the ad-hoc and idiosyncratic processes by replacing particular choices with a formulaic and principled set of steps for one to follow when processing data from a study. What this process also incentivize is that researchers use specialized tools designed to be used for the distinct steps of the process. As the next chapter will grapple with more, moving away from using a single tool for all steps of the research process encourages one to follow the best practices for each step as these tools' capabilities are a reflection of the widely-accepted best practices for that particular task.

## A principled data management workflow

@fig-workflow provides an overview to my proposed workflow to data management. The remainder of this chapter will cover each of these steps and explain what the value added is in terms of security, replicability, and efficiency by using this workflow.

{{< include ../assets/_workflow.qmd >}}

In **Step 1**, you should make sure to download a `.csv` file if possible. Why a `.csv` file specifically? Other file types such as STATA's `.dta`, Microsoft Excel's `.xlsx` and other common ones often have some degree of proprietary protection of them and one's ability to access that raw data is dependent on these companies' continued willingness to allow for users to develop packages that allow for one to load that file. 

Further, as I will advocate for in the next chapter, one should use the `SQL` language for their data management as it is designed for such a task and therefore provides incentives to follow best practices. `DuckDB`'s implementation of `PostgreSQL` enables one to create tables from a raw data `.csv` file and to initiate a new `.duckdb` file without having to open the original data file or loading it with some other language. This reduces the chances that one makes changes to the data that are not documented either through automatic formatting that sometimes occurs by Microsoft Excel, Apple Numbers, or some other software when parsing a `.csv` file to make it more readable in a tabular format as well as reducing the temptation to perform ad-hoc data management by loading the file into R, Python, Julia or some other language for data analysis.

Once you have downloaded the raw data as a `.csv` file, in you can immediately load and interface with it using `DuckDB`. Once you have done this, in **Step 2** you should create a case (participant) identifier column. This can be as simple as creating a column that records the current row number for the participant. As you will see based on my recommendations in the next few steps, the goal of this is that you should decentralize your data so that you ensure that you do not accidentally share information that makes it easy for people to identify your participants, while also not deleting data so that, internally, you may have access the original data in its complete form.

In **Step 3** you can start creating and storing tables in your `.duckdb` file. You should pull any identifiable data from the loaded data and store it in a separate table. This sort of information would certainly be columns containing the names and addresses (mailing or IP), Session ID's, Time and/or location they took the study from, as well as any Participant ID provided by a vendor. With this subset of your original data, you can store a table with a copy your Participant ID column (from **Step 2**). 

Keeping these data allow you to immediately de-identify your data while also not deleting it so that you are able to continue to use any of that data in case you need to apply exclusion restrictions, use those data for payment to participants, confirm participation in a study, etcetera. The generated Participant ID column allows you to merge data from that table with identifiable information if need be at a later time, but ensures that you do not have *any* data that may make it easy to identify your participants in an analysis or any of your files that you make publicly available.

Taking this step early on not only helps with ensuring the confidentiality of your participants, but doing it this way helps with replicability in that you will have to write `SQL` code that documents every step you took from downloading the file on your computer through your analysis. It also aids in efficiency in that you will still have those data that you can easily merge with the main subset of your data stored in the second table in case you need to when figuring out which participants to exclude from the study, confirm whether participants completed the study and are eligible for compensation, etcetera. Also, if internally, you need to demonstrate that your data is intact, then you can easily merge on your generated Participant ID column.

Once you have pulled out the identifiable information about your subjects that you want to keep separate from the subset you will use for your analyses, you will want to store both as separate tables in your `.duckdb` file. So, by the end of **Step 4**, you should have a `.duckdb` file saved on your computer that contains two tables: a table with your Participant ID column (**Step 2**) that contains identifiable information about your participants, and a second table that contains the same Participant ID column (**Step 2**) with the data that you will use for your analyses.

The benefit of having a `.duckdb` file rather than separate files or a single excel file with multiple sheets containing the same amount of information is that if there is unauthorized access to your computer a `.duckdb` file requires that someone write `SQL` code in order to view any of the contents of the file. This increases security in the event that someone gains access to the file. Obviously if the person with unauthorized access to the file understands `SQL` they can access the data. However, the architecture of the file (having multiple tables and knowing which column contains what information and what that information represents) also requires knowledge about the file that should be ideally stored as some sort of codebook or separate documentation. The increased requirements for technical know-how and of internal documentation of the "schema" (the structure of the tables within the file) significantly increases the complication of viewing the data for those that are not part of the research team, thus increasing security and your ability to retain the confidentiality of your subjects' identities.

Once we have completed **Step 4**, we can begin to write `SQL` code to clean our data in **Step 5**. While this asks researchers to eschew packages and languages that many academics have become accustomed to using, as I argue in the next chapter, it ensures that we are using the right tool for the task. `SQL` is a language designed with the express purpose of data management. Because of its specialization, this means that the language's capabilities, design, and workflow are optimized for this particular task. There is also something to be said about the psychological benefits of using different tools for different steps of the research process -- it encourages you to consider each step as complete and separate, therefore encouraging better documentation. 

As I will discuss more in the following chapter as well, many of the languages that researchers use for data analysis (and as a consequence default to using the same language for data management), have very large and very popular packages for data management. While these packages are large, open source, and have teams of professional developers who consistently seek to increase the functionality and efficiency of the functions in those packages, they also change rapidly by changing names of functions, implementing new functions, depreciating functions, and make adjustments to how those functions work. These characteristics are the main sources of criticism towards using them as they create many headaches for researchers as new versions of these packages often create new errors or depreciation warnings that make it harder to be confident in the continued replicability of one's code. There are also other criticisms to some of these packages as they are very reliant on other functions (have a relatively higher number of dependencies) and if those packages that they depend upon change or are no longer supported, this creates problems as well as it can limit how fast one's code runs as well as the replicability of one's code.

These features are reflective of what these languages are often built for: data analysis. As we innovate in the models and techniques available to us to detect patterns in our data, the need for constant integration of these new tools encourages developers and applied users to adjust our code to reflect these changes in standards and techniques.

`SQL`, however, is a quite old language (in terms of coding languages existing today). It also has not changed much. There are two primary flavors of `SQL` such as the open-source `PostgreSQL` that `duckdb` relies upon. Given that most needed innovations in data management are focused on a user's ability to access an increasingly larger volume of data in a shorter amount of time, these demands do not require or incentivize changes to the language's functions that a researcher would use. As a result, we do not need to worry about our `SQL` code needing to be updated in response to changes to function names or any depreciations. The only thing that really changes are the versions of the packaged software that we interface with, but the underlying code does not because the developers are responding to demands to make the database file sizes smaller, for them to load faster, and to do so with ballooning datasets; but the common need to select certain columns, rename them, use row-wise and column-wise aggregations, etc. do not change.

If I have successfully convinced you to do the data cleaning with `SQL` rather than using a package in a language for data analysis, we will want to save a *third* table that contains the cleaned data you will use in your analyses. From there, in **Step 6** we can begin to perform our analyses by loading this third, cleaned, table of the data for our analyses.

After we have completed our analyses and we are preparing our paper for submission to a journal or to share it publicly, we will want to perform **Step 7** which means that we will create and save a new `.duckdb` file. This second `.duckdb` file will store a copy of the cleaned and de-identified data table(s) we use in our analyses as well as the table containing the original and de-identified data.

## Conclusions

I hope with this chapter I have given you a sense of my proposed workflow. You still may not be convinced that this is workflow is better or that it is more worth doing that what you currently do. After all, I am encouraging you to eschew packages and languages that you are comfortable with and probably have used for quite a while in favor of one that may seem unnecessary. That is fine. As our data management workflows are so idiosyncratic, it is not easy nor prudent to compare every single workflow academics have with data management to the one that I propose here. Therefore, it is absurd to claim that this proposed workflow is better than all possible workflows. 

Nevertheless, my primary goal of this chapter is to introduce this workflow and to plant the seeds about how having *any* principled and formulaic workflow for managing data is a vast improvement to any *ad hoc* approach in terms of security, reproducibility, and efficiency. I also hope that the present chapter makes it clear that researchers are stuck in this weird position of having to balance concerns about security, replicability, and efficiency that seem at odds with one another. Instead of having to chose which of these goals to pursue with our projects, I hope that this chapter demonstrated how my proposed workflow threads the needle by reaching all three of these goals simultaneously.

The primary goal of the following chapter is to encourage the uptake of this particular workflow and its tools -- or at least to be used as a skeleton workflow that you make amendments to for your particular needs. I hope to show what this workflow looks like in practice and to demonstrate how painless it is to implement. I also hope that the following chapter demonstrates how the particular tools I advocated to use in this workflow are worth learning and using. Therefore, the following chapter will contain much more code.