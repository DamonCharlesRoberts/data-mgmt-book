# A principled workflow for secure and replicable data management

There are two key issues that we have to grapple with when it comes to data management. The first is that we need to ensure that our data are secure for the protection of our subjects. The second is that we need to ensure that our original data remain intact and any changes need to be documented for the purposes of replicability.

In the United States, Institutional Review Boards (IRBs) are tasked with the responsibility of enforcing federal regulations that require that researchers keep their data secure and that they protect the confidentiality of research subjects. While enforcement of these regulations can vary to some extent, this often means that we need to ensure that our data stays stored on a secure computer or locked away if they are physical files. While this is relatively easy to do and protects the confidentiality of our research subjects, this butts up against increasing calls that we make our research replicable at every step of the process which often means that we need to share de-identified data with journals upon publication of our papers.

Resulting from a significant number of high-profile cases of famous academic papers and researchers who have been accused of mis-handling their data, there are increasingly higher expectations for researchers to provide documentation and materials that allow researchers to independently re-do your study. This occurs as the social sciences currently grapple with a "Replication Crisis" where there are many papers being retracted for a number of issues with the empirical evidence presented in them. 

This creates a very tricky situation for researchers. We are expected to ensure that others cannot identify any of our participants while it is extremely hard to verify the validity of a researchers claims without being able to verify that the authors took the proper and reported steps at each and every stage of the process. One reason this is so tricky is because there is no concrete set of standards or best practices that researchers follow when it comes to data management.

While we receive extensive training and spill a significant amount of ink about where to collect data, what data to collect and how to analyze them, we receive little-to-no guidance in what to do with the data between the steps of collecting and analyzing those data. We follow IRB regulations by storing our data on a password protected computer and then make judgment calls about what subset of our data to share publicly with the journal and readers of our papers. Therefore we have non-standardized processes for documenting the steps to de-identify and clean our data. This naturally leads to very idiosyncratic workflows for data management and reflects the extent to which these steps are an afterthought in the research process. The goal of this chapter is to convince you that the current state of data management in the social sciences is indeed poses a problem and that we do not have to be this way. 

Let me provide a couple of case studies of the current way that people manage their data.

<!--
Note:

- In the following bits discussing these case studies use the first para. to discuss these processes, then in a second dissect what potential problems there are with them.

-->
**Case Study 1:** You download a `.csv` (or `.xlsx`, `.dta`, or even a `.sav`) file from the site that you hosted your study and collected your data on. You then open the file and start relabeling things. You may have a column that originally comes in something like "VAR_001" and you change the column name to "PARTICIPANT_ID". You also notice that some of the variables have rows with the label of the response rather than an integer. So you start changing any cell that says "Strongly Disagree" to "1". Once you have gone through, this you save a new copy of the file and call it "cleaned-data.csv". Then you load the file and start performing descriptive statistics and fit simple regressions to get a preliminary sense of whether your study paid off. If you find any mistakes, you open the "cleaned-data.csv" file again to fix them or go back to the original file and restart the cleaning process. Or you may even write code to correct the mistakes in the "cleaned-data.csv" file rather than having to deal with fixing those mistakes by restarting or trying to find them in a few-hundred row spreadsheet.

There are a number of things that are dangerous to this process. The first is that your data management and data pre-processing is not at all replicable. 

The goal of replicability is that someone else can take your original data, and they can follow step-by-step what you did to come to the exact same conclusions. Manually editing the original file does not populate any documentation about what things you are clicking or typing. There is so little documentation, that you might not even be able to replicate the steps you took to manage and pre-process your data; let alone someone else. Even scarier, your finger may slip and you type the wrong number or press the delete key while going over the wrong cell. Sometimes you catch this, but sometimes you may not. Without this documentation, if another researcher finds discrepancies there can only be speculation as to whether those discrepancies arose from research design choices or data management issues. Not only can this carry professional costs, but it also limits our ability to be confident that we understand the true answer to your question.

The other problem is that these steps are highly inefficient. Manually going through cells on a spreadsheet may take a few minutes to a few hours to clean if you have a few hundred rows with a dozen or so columns. However, many of the data sets that we use in the social sciences are often approaching thousands (if not millions) of rows and hundreds of columns. To do this for such a large dataset would take a significant amount of time and any mistakes would be extremely hard to catch in such a vast amount of data.

The most likely thing that will happen when asked to share your data with a journal upon publication of your research is that you will just share the cleaned excel file. Why? Well, because you are not going to want to reinvent the wheel. At this step, it can also be quite dangerous. Not only does this cause the lack of replicability of your analysis, but you may forget to remove identifiable information from the file. If this happens, you have failed to meet federal regulations requiring that you take every effort to ensure your participants are not easily identifiable.

**Case Study 2:** Similar to Case Study 1, you download your file. Instead, you open up an R, STATA, or Python session. You start writing code in a cleaning script that renames the columns, and recodes columns to have the correct integer values. You then start writing code to perform descriptive statistics and to fit simple regressions for preliminary analyses. While doing this, you may notice some mistakes and so you go back to your code and make some adjustments to erase those mistakes.

This process is a little bit more replicable and efficient than the first one. Writing code allows you to systematically make changes to any cell where some set of conditions are applied. The code also does this relatively quickly (in many cases). This also is safer (in terms of replicability) as the code acts as your documentation as to what you changed from the original dataset that you had. 

While this is a vast improvement over the first case, there are still some aspects of this that are relatively dangerous. The first is that the replicability and accuracy of your documentation is still heavily dependent on whether you write your code as a script or run the code line-by-line (interactively). Running your code as a script means that you run the whole file at once rather than running it line-by-line (interactively). Executing this code as a script is much more replicable because it does not allow you to cherry-pick which lines of code you execute, but it requires your code to be clean and to only execute what you intend to execute. The second is that there is no clear step to ensure that your data is de-identified to ensure that you do not accidentally share data that would betray the identity of your subjects. Even still, if you take steps to de-identify the data, you cannot include that code in your script or else it will throw errors back at an independent researcher that only has the de-identified version of the data. So this still means that there are a high number of researcher degrees of freedom by which some researchers can be more or less compliant with protecting the anonymity of their subjects as well as providing documentation for replicating one's analyses.

In the rest of this chapter I will introduce and advocate for a workflow that should enable replicable, efficient, and secure files for one's research. As I will argue, this process removes much of the ad-hoc and idiosyncratic processes by replacing particular choices with a formulaic and principled set of steps for one to follow when processing data from a study. What this process also incentivize is that researchers use specialized tools designed to be used for the distinct steps of the process. As the next chapter will grapple with more, moving away from using a single tool for all steps of the research process encourages one to follow the best practices for each step as these tools' capabilities are a reflection of the widely-accepted best practices for that particular task.

## A principled data management workflow

@fig-workflow provides an overview to my proposed workflow to data management. The remainder of this chapter will cover each of these steps and explain what the value added is in terms of security, replicability, and efficiency by using this workflow.

{{< include ../assets/_workflow.qmd >}}

In **Step 1**, you should make sure to download a CSV file if possible. Why a CSV file specifically? Other file types such as STATA's `.dta`, Microsoft Excel's `.xlsx` and other common ones often have some degree of proprietary protection of them and one's ability to access that raw data is dependent on these companies' continued willingness to allow for users to develop packages that allow for one to load that file. 

Further, as I will advocate for in the next chapter, one should use the `SQL` language for their data management as it is designed for such a task and therefore provides incentives to follow best practices. DuckDB's implementation of `PostgreSQL` enables one to create tables from a raw data `CSV` file and to initiate a new `.duckdb` file without having to open the original data file or loading it with some other language. This reduces the chances that one makes changes to the data that are not documented either through automatic formatting that sometimes occurs by Microsoft Excel, Apple Numbers, or some other software when parsing a `CSV` file to make it more readable in a tabular format as well as reducing the temptation to perform ad-hoc data management by loading the file into R, Python, Julia or some other language for data analysis.

Once you have downloaded the raw data as a `CSV` file, in you can immediately load and interface with it using `DuckDB`. Once you have done this, in **Step 2** you should create some sort of case (participant) identifier column. This can be as simple as creating a column that records the current row number for the participant. 

In **Step 3** you can start creating and storing tables in your `.duckdb` file. You should pull any identifiable data from the loaded data and store it in a separate table. This sort of information would certainly be columns containing the names and addresses (mailing or IP), Session ID's, Time and/or location they took the study from, as well as any Participant ID provided by a vendor. With this subset of your original data, you can store a table with a copy your Participant ID column (from **Step 2**). Keeping these data allow you to immediately de-identify your data while also not deleting it so that you are able to continue to use any of that data in case you need to apply exclusion restrictions, use those data for payment to participants, confirm participation in a study, etcetera. The generated participant id column allows you to merge data from that table with identifiable information if need be at a later time, but ensures that you do not have *any* data that may make it easy to identify your participants in an analysis or any of your files that you make publicly available.

Taking this step early on not only helps with ensuring the confidentiality of your participants, but doing it this way helps with replicability in that you will have to write `SQL` code that documents every step you took from downloading the file on your computer through your analysis. It also aids in efficiency in that you will still have those data that you can easily merge with the main subset of your data stored in the second table in case you need to when figuring out which participants to exclude from the study, confirm whether participants completed the study and are eligible for compensation, etcetera. Also, if internally, you need to demonstrate that your data is intact, then you can easily merge on your generated participant id column.

Once you have pulled out the identifiable information about your subjects that you want to keep separate from the subset you will use for your analyses, you will want to store both as separate tables in your `.duckdb` file. So, by the end of **Step 4**, you should have a `.duckdb` file saved on your computer that contains two tables: a table with your participant id column (**Step 2**) that contains identifiable information about your participants, and a second table that contains the same participant id column (**Step 2**) with the data that you will use for your analyses.

Once we have completed **Step 4**, we can begin to write `SQL` code to clean our data in **Step 5**. Once we have written this code to do it we will want to save a *third* table that contains the cleaned data you will use in your analyses. From there, in **Step 6** we can begin to perform our analyses by loading this third, cleaned, table of the data for our analyses.

After we have completed our analyses and we are preparing our paper for submission to a journal or to share it publicly, we will want to perform **Step 7** which means that we will create and save a new `.duckdb` file. This second `.duckdb` file will store a copy of the cleaned data table(s) we use in our analyses as well as the table containing the original and de-identified data.
