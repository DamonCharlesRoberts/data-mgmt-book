{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The basics of data extraction"
      ],
      "id": "305e8913"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: python-setup-block\n",
        "#| include: false\n",
        "\n",
        "# Load packages\n",
        "\n",
        "import timeit\n",
        "import duckdb as db\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "from statistics import mean, stdev\n",
        "from random import seed\n",
        "\n",
        "# Set seed\n",
        "\n",
        "seed(121022)\n",
        "\n",
        "# Connect to database\n",
        "\n",
        "con = db.connect('../data/2020-anes/anes.db')\n",
        "\n",
        "# Load the spatial extension for DuckDB\n",
        "con.execute(\n",
        "    '''\n",
        "    INSTALL spatial; -- only need to run this once per connection\n",
        "    LOAD spatial; -- only need to run this once per connection\n",
        "    '''\n",
        ")"
      ],
      "id": "python-setup-block",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{r}\n",
        "#| label: r-setup-block\n",
        "#| include: false\n",
        "# Load libraries\n",
        "\n",
        "library(tictoc)\n",
        "library(readr)\n",
        "library(haven)\n",
        "library(foreign)\n",
        "library(readxl)\n",
        "library(arrow)\n",
        "```\n",
        "\n",
        "\n",
        "The last chapter presented an overview of a principled workflow for data processing in a research project. I include a diagram of this workflow in @fig-workflow for your reference. This chapter will focus on the first part of the workflow: data extraction. When discussing data extraction, I am referring to the idea that you are taking raw data outputs from some source and are converting it into a usable tabular format. In this chapter, we are going to cover common data formats such as CSV and TSV files, Parquet files, and proprietary file types such as Microsoft's XLSX, STATA's DTA, and SPSS' SAV files."
      ],
      "id": "9b485207"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "format: html\n",
        "---"
      ],
      "id": "4c23eb5c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{mermaid}\n",
        "%%| label: fig-workflow\n",
        "%%| fig-cap: A principled workflow for data management\n",
        "%%| fig-width: 6.5\n",
        "%%{init: {'theme':'base', 'themeVariables':{'primaryColor':'#ffffff', 'primaryBorderColor': '#000000'}}}%%\n",
        "flowchart TB\n",
        "    subgraph s1[Data Extraction]\n",
        "        direction TB\n",
        "        A[Download CSV file] --> B[Create Participant ID Column]\n",
        "        B --> C[Pull out identifiable data -- keep Participant ID column in each subset of data]\n",
        "        C --> D[Store both subsets to two tables in a DuckDB file]\n",
        "    end\n",
        "    subgraph s2[Data Transformation]\n",
        "        direction TB\n",
        "        E[Clean data in stored DuckDB file and store as new tables -- do not overwrite original two tables]\n",
        "    end\n",
        "    subgraph s3[Data Loading]\n",
        "        direction TB\n",
        "        F[Load data to perform analyses from updated tables in DuckDB file]\n",
        "    end\n",
        "    subgraph s4[Data Distribution]\n",
        "        direction TB\n",
        "        G[Create a new DuckDB file with a copy of the cleaned tables used in the analyses and the table with de-identified original data]\n",
        "        G --> H[Share new DuckDB file with copied tables along with SQL script used for the cleaning of the data when submitting to journal]\n",
        "    end\n",
        "    s1 --> s2\n",
        "    s2 --> s3\n",
        "    s3 --> s4\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Though I recommend that you perform many of these tasks with `DuckDB`'s implementation of `SQL` goal of this chapter is to give you some examples of performing these steps with code in `Python`, and `R` as well. The objective is to encourage researchers to use this principled data processing strategy, even if I am unable to convince everyone to use `SQL`. I also will provide coding benchmarks to demonstrate the efficiency of `DuckDB`'s implementation of `SQL` relative to the common libraries in `Python`, and `R`.\n",
        "\n",
        "## Data extraction\n",
        "\n",
        "What is data extraction? Data extraction is a task that every researcher engages in. While many may think that it is a easy process, the reality is that in *some* cases data extraction is an easy process. When working with pre-compiled datasets, data extraction can certainly be an easy process. You can download some file and then load it into your statistical software of choice. However, even when you have a pre-compiled dataset, this process can still be quite challenging.\n",
        "\n",
        "As the scale and the number of types of data that social scientists consider for their projects increases, data extraction steps are less standard and are orders of magnitude more complicated. In this chapter, we are focusing on data sets that have already been compiled, are in a tabular format, and are relatively small (thousands of rows and a couple of hundred columns). However, when we are working with raw data that are not in tabular format already -- such as reading text from PDF files -- or contains millions of observations for thousands of variables, data extraction is an extremely difficult process and choosing a tool that is responsive to scalability and facilitate documentation are imperative. The next chapter will provide examples of some of these more complicated features. However, the remainder of this chapter will be focused on providing simple and common examples of the data that social scientists will want to extract.\n",
        "\n",
        "## Extracting data from Separated-Value files\n",
        "\n",
        "Separated-value files are quite common for researchers. These files refer to those where we separate the different cells (or pieces of data) by some delimiter. The most common that we interact with are CSV files which refer to a separated-value file where the cells are separated by commas, `,`. But, there are many others. Some of us may have experience with tab separated-value files, `TSV`.\n",
        "\n",
        "Loading separated-value files is a common data extraction task that many experienced researchers are familiar and comfortable with. Often times, to execute  the code for data extraction takes milliseconds once you have the dataset's separated-value file downloaded on your computer for many common datasets we encounter in the social sciences.\n",
        "\n",
        "Since CSV files are such a common file to extract our data from, let's start there. The CSV file I am working with in these examples is a dataset that has recorded the amount of Waffle Houses in a particular state and information about the rate of marriages and divorces in that same state. This is a relatively small dataset as it is $50\\times13$ (50 rows and 13 columns). I've included this CSV file in the supplementary materials for the book online. For those interested in going straight to the source for the data, you should reference @mcelreath_statistical_2020.\n",
        "\n",
        "### `Python`\n",
        "\n",
        "In `Python` there are two dataframe management libraries that I will consider. Of course, there are many more than this, but here I want to focus on `Pandas` which is by far one of the most popular libraries in `Python`. I will also discuss `Polars` which is newer and not as adopted as `Pandas`. The benefit of looking into `Polars` is that it is extremely performant. The `Polars` library is a API for `Polars` which is written natively in `Rust` (an increasingly popular alternative to compiled languages such as `C++`).\n",
        "\n",
        "Once I downloaded the dataset, I put it in my project folder called `data/`. I first want to specify the location of that particular file. It is not sufficient for me to just tell my computer to look for a file called `WaffleDivorce.csv` because it would take an extremely long time for my computer to look through millions of files for a matched name. The other problem of not being a bit more specific about the location of the file is that I may have multiple files on my computer called `WaffleDivorce.csv` but for other projects. If you have ever looked through your computer and have found multiple files called `Untitled1.docx`, this is a pretty common experience that many of us have. Just as you'd be confused if you need to open `Untitled1.docx` on your Desktop or in a specific folder somewhere else on your computer, so is your computer if you aren't a bit more specific about *where* on your computer to find the file. So, in this case, I am going to tell it to find the file in my current project (`./`) that is within my `data/` folder. So, the specific path to the CSV is `./data/WaffleDivorce.csv`. On line 2, you will see that I store this path for repeated use in an object called `csv_path`.\n",
        "\n",
        "Once, I have specified the path to my file by placing it in an object called `csv_path`, I can then load the `Pandas` library by writing `import pandas` as you see on line 6. I have an extra step on that same line where I add `as pd` which allows me to load `Pandas` but to give it a nickname so I do not have to write out `pandas` to then access a function.\n",
        "\n",
        "So as you see on line 7 where I load the csv file, I write `pd.read_csv`. Which means that I am using the function `read_csv` from the `pandas` library. But, since I had used `as pd`, I didn't have to write `pandas.read_csv`.\n",
        "\n",
        "On line 8, I write the code that will load my CSV file. I tell `Python` to use the `read_csv` function from `Pandas` where I use my `csv_path` object as an argument to the function to tell it which file to load. I also specify another argument for the object. This argument is a named argument called `delimiter`. This argument allows me to specify what in the file defines separation between cells. In this case, the file is stored as a CSV file, but it actually uses semi-colons, `;`, as the separator rather than a comma, `,`. Under the hood, `Pandas` is taking the data from the `WaffleDivorce.csv` file and is extracting it and then converting it into a `Pandas` `DataFrame` object by keying in on the delimiter that I specify. There are many other arguments that you can pass to the `Pandas` `read_csv` function that helps you work with separated-value files that may have other strange characteristics, but I recommend you looking at `Pandas`' official documentation to explore your options. Once loaded and placed in a `Pandas` `DataFrame` object, the object is then linked to the object `anes_df` which allows me to refer back to the `Pandas` `DataFrame` later so that I do not have to re-extract the data later.\n"
      ],
      "id": "3826334b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: python-load-waffle-house\n",
        "#| eval: false\n",
        "# Define location of csv file\n",
        "csv_path='./data/2020-anes/anes.csv'\n",
        "\n",
        "# Pandas\n",
        "    #* Import Pandas\n",
        "import pandas as pd\n",
        "    #* Load CSV\n",
        "anes_df = pd.read_csv(csv_path, delimiter=';')\n",
        "\n",
        "# Polars\n",
        "    #* Import Polars\n",
        "import polars as pl\n",
        "    #* Load CSV\n",
        "anes_df = pl.read_csv(csv_path, separator=';')\n",
        "\n",
        "# Preview of dataframe\n",
        "anes_df.head()"
      ],
      "id": "python-load-waffle-house",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I perform the same task but this time I extract the data and place it into a `Polars` `DataFrame` instead of pandas. On line 12, I import the `Polars` library and give it the nickname of `pl` similar to how I did with `pandas`. Notice how I do not give them the same nickname. I do this to be careful about confusing `Python` about whether I am referring to `Pandas` or `Polars`.\n",
        "\n",
        "On line 14, I extract the data and place it into a `Polars` dataframe with the `read_csv` function from the `Polars` library. In this case, `Polars` and `Python` both call their function `read_csv` if you are loading a separated-values file and are trying to place it into one of their `DataFrames`. As a side note, this is a problem that common users of `R` face where many are often much less explicit about the particular function they want to use than you are in `Python`. But in `Python`, we know that the `read_csv` functions are different and should produce two different types of `DataFrame` objects, `Pandas` or `Polars`, because we append the nickname of the library to the function name (e.g. `pd.read_csv` versus `pl.read_csv`).\n",
        "\n",
        "We see another difference on line 14. With `Polars`' `read_csv` function, I use the named argument `separator` as opposed to `delimiter`. These diverging choices in what to name these arguments for their `read_csv` functions are choices that the developers of the libraries chose. But, they do the same thing. The `separator` argument specifies what value separates the cells from each other. Which with the `WaffleDivorce.csv` file was a semi-colon, `;`.\n"
      ],
      "id": "1a5b4b1a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: profile-python-waffle-house-load\n",
        "#| include: false\n",
        "# Define path for csv file\n",
        "csv_path='../data/2020-anes/anes.csv'\n",
        "# Pandas\n",
        "    #* Define a function to be ran through\n",
        "def pandas_extract_csv(path=csv_path):\n",
        "    anes_df = pd.read_csv(path, delimiter=';')\n",
        "\n",
        "    #* Repeat through the defined function 100 times and make list of time taken\n",
        "pandas_load_time = timeit.repeat(\n",
        "    'pandas_extract_csv()', # run the function\n",
        "    'from __main__ import pandas_extract_csv', # after importing it\n",
        "    number=1, # run through it once\n",
        "    repeat=100 # but repeat this 100 times\n",
        ")\n",
        "\n",
        "# Polars\n",
        "    #* Define a function to be ran through\n",
        "def polars_extract_csv(path=csv_path):\n",
        "    anes_df = pl.read_csv(path, separator=';')\n",
        "\n",
        "    #* Repeat through the defined function 100 times and make a list of time taken\n",
        "polars_load_time = timeit.repeat(\n",
        "    'polars_extract_csv()', # run the function\n",
        "    'from __main__ import polars_extract_csv', # after importing it\n",
        "    number = 1, # run through it once\n",
        "    repeat=100 # but repeat this 100 times\n",
        ")"
      ],
      "id": "profile-python-waffle-house-load",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Keep in mind that this is the same file but I am extracting data from it with two different libraries and it produces two different types of `DataFrames`. The first is a `Pandas` `DataFrame` and the second is a `Polars` `DataFrame`. So, not only are the ways in which these packages extract the data different from each other but the resulting `DataFrames` are different. They are not different in that the contents of the tables are different, but rather the fundamentals of the table are different. As we will discuss in the chapters about data transformation, this has a pretty significant impact on how we think about transforming and interacting with our data in terms of what our code looks like and the performance of our code.\n",
        "\n",
        "But, to focus on differences in the performance of the data extraction task, I wanted to examine the speed at which these two libraries load the file, transform the data into a tabular format, and then assign it to a callable `DataFrame` object. For this particular file, the speed at which this happens is extremely quick because we are dealing with a small amount of data (50 rows and 13 columns). However, when we work with larger datasets that can be thousands of rows and hundreds of columns, the speed at which this data extraction occurs can decrease significantly. Importantly, the differences between both of these packages can also diverge. That is, `Polars` claims to have smaller decreases in speed relative to `Pandas` as we are operating with larger datasets. These differences between file sizes and between libraries increases significantly the larger these datasets become. But for now, I look at this particular dataset and how quickly these two libraries can extract the data.\n"
      ],
      "id": "94b2f964"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "#| echo: false\n",
        "print(\n",
        "    f'I use the `timeit` package to calculate the average amount of time it took `Pandas` and `Polars` to load the WaffleDivorce.csv file 100 times. It took `Pandas` on average {mean(pandas_load_time):.2e} (Std. Deviation = {stdev(pandas_load_time):2e}) seconds while it took `Polars` an average of {mean(polars_load_time):.2e} (Std. Deviation = {stdev(polars_load_time):.2e}) seconds.'\n",
        ")"
      ],
      "id": "74a5877f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I am able to independently replicate `Polars`' claims that their library loads data quicker than `Pandas` does. The average differences in speed are extremely small. The results of this profiling (test of how long a set of code took to execute) are reported in scientific notation. However, as mentioned before, as we have more data we should expect the time it takes to load these files will increase for both `Pandas` and `Polars`, as well as the differences in the time between the two libraries. But demonstrating that latter comparison will be a task for the next chapter. Let us look at data extraction in `R`.\n",
        "\n",
        "### `R`\n",
        "\n",
        "\n",
        "```{r}\n",
        "#| label: r-waffle-house-load\n",
        "#| eval: false\n",
        "# Define path to CSV\n",
        "csv_path <- './data/2020-anes/anes.csv'\n",
        "# Base R\n",
        "    #* Load CSV file\n",
        "anes_df <- read.csv(csv_path, sep=';')\n",
        "\n",
        "# Tidyverse\n",
        "    #* Load readr\n",
        "library(readr)\n",
        "    #* Load CSV file\n",
        "anes_df <- read_delim(csv_path, delim=';')\n",
        "\n",
        "# Preview dataframe\n",
        "head(anes_df)\n",
        "```\n",
        "\n",
        "```{r}\n",
        "#| label: profile-r-waffle-house-load\n",
        "#| include: false\n",
        "# Define path to csv file\n",
        "csv_path <- './data/2020-anes/anes.csv'\n",
        "\n",
        "# Base R\n",
        "    #* Clear tictoc log\n",
        "tic.clearlog()\n",
        "    #* Profiling each step of this process\n",
        "for(i in 1:100) { # repeat the following 100 times\n",
        "    tic(i) # start timer\n",
        "    anes_df <- read.csv(csv_path, sep=';') # load the csv\n",
        "    toc(i, quiet=TRUE) # stop timer\n",
        "}\n",
        "base_r_csv_benchmark_log <- tic.log(format=FALSE) # grab the log of how long this took\n",
        "base_r_csv_benchmark <- unlist( # and split this log up\n",
        "    lapply(\n",
        "        base_r_csv_benchmark_log # by repeatedly taking the entries in the log\n",
        "        , function(x) x$toc - x$tic # and subtracting the start time from the stop time for each iteration\n",
        "    )\n",
        ")\n",
        "\n",
        "# Tidyverse\n",
        "    #* Clear tic toc log\n",
        "tic.clearlog()\n",
        "    #* Profiling each step of this process\n",
        "for(i in 1:100) { # repeat the following 100 times\n",
        "    tic(i) # start timer\n",
        "    anes_df <- read_delim(csv_path, delim=';', show_col_types=FALSE) # load the file time\n",
        "    toc(i, quiet=TRUE) # stop timer\n",
        "}\n",
        "tidyverse_csv_benchmark_log <- tic.log(format=FALSE) # grab the log of how long this took\n",
        "tidyverse_csv_benchmark <- unlist( # and split this log up\n",
        "    lapply(\n",
        "        tidyverse_csv_benchmark_log # by repeatedly taking the entries in the log\n",
        "        , function(x) x$toc - x$tic # and subtracting the start time from the stop time for each iteration\n",
        "    )\n",
        ")\n",
        "```\n",
        "\n",
        "```{r}\n",
        "#| output: asis\n",
        "#| echo: false\n",
        "cat(\n",
        "    \"I use the `tictoc` package to calculate the average amount of time it took `Base R` and the `Tidyverse` package to extract data from the anes.csv file and to generate a `DataFrame` 100 times. It took `Base R` an average of \", format(mean(base_r_csv_benchmark), digits=3, scientific=TRUE), \" (Std. Deviation = \", format(sd(base_r_csv_benchmark), digits=3, scientific=TRUE), \") seconds per attempt, while it took `Tidyverse` an average of \", format(mean(tidyverse_csv_benchmark), digits=3, scientific=TRUE), \" (Std. Deviation = \", format(sd(tidyverse_csv_benchmark), digits=3, scientific=TRUE), \") seconds per attempt.\"\n",
        "    , sep=\"\"\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "### `DuckDB`\n"
      ],
      "id": "ac8288cc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: sql-load-waffle-house\n",
        "#| eval: false\n",
        "# Import the DuckDB library\n",
        "import duckdb as db\n",
        "\n",
        "# Use the Python API for DuckDB to connect to the database\n",
        "con = db.connect('./data/2020-anes/anes.db')\n",
        "\n",
        "# Use the connection to extract the data from the csv and place it as a table with the DuckDB API\n",
        "con.execute(# Execute the following SQL query ...\n",
        "    '''\n",
        "        CREATE OR REPLACE TABLE -- create or replace a table\n",
        "            main -- ... called main\n",
        "        AS -- and place the following data in the main table\n",
        "            SELECT -- select the following columns\n",
        "                *  -- all of them\n",
        "            FROM -- from ...\n",
        "                read_csv_auto('./data/2020-anes/anes.csv') -- this specific file\n",
        "        /*\n",
        "            NOTE:\n",
        "                - I am using a function by DuckDB called read_csv_auto \n",
        "                  that extracts the data for me. There are some\n",
        "                  alternatives available if you use PostgreSQL or MySQL.\n",
        "                - I specify the file path inside the read_csv_auto function.\n",
        "                  When I do this, I use single quotation marks.\n",
        "        */\n",
        "    '''\n",
        ")"
      ],
      "id": "sql-load-waffle-house",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: sql-profile-waffle-house-load\n",
        "#| include: false\n",
        "# Polars\n",
        "    #* Define a function to be ran through\n",
        "def duckdb_extract_anes():\n",
        "    con.execute(# Execute the following SQL query ...\n",
        "        '''\n",
        "            CREATE OR REPLACE TABLE -- create or replace a table\n",
        "                main -- ... called main\n",
        "            AS -- with the following data\n",
        "                SELECT -- select the following columns\n",
        "                    * -- all of them\n",
        "                FROM -- from ...\n",
        "                    read_csv_auto('../data/2020-anes/anes.csv') -- this specific file\n",
        "            /*\n",
        "                NOTE:\n",
        "                    - I am using a function by DuckDB called read_csv_auto \n",
        "                      that extracts the data for me. There are some\n",
        "                      alternatives available if you use PostgreSQL or MySQL.\n",
        "                    - I specify the file path inside the read_csv_auto function.\n",
        "                      When I do this, I use single quotation marks.\n",
        "            */\n",
        "        '''\n",
        "    )\n",
        "    #* Repeat the defined function 100 times and make a list of time taken\n",
        "duckdb_load_time = timeit.repeat(\n",
        "    'duckdb_extract_anes()', # run this function\n",
        "    'from __main__ import duckdb_extract_anes', # after importing it\n",
        "    number = 1, # and run the function once\n",
        "    repeat = 100 # but repeat this process 100 times\n",
        ")"
      ],
      "id": "sql-profile-waffle-house-load",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "#| echo: false\n",
        "print(\n",
        "    f\"I use the `timeit` package to calculate the average amount of time it took the `DuckDB` API in `Python`  to load the main table from the WaffleDivorce.DB 100 times. It took `DuckDB` on average {mean(duckdb_load_time):.2e} (Std. Deviation = {stdev(duckdb_load_time):.2e}) seconds.\"\n",
        ")"
      ],
      "id": "38407dd3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting data from Parquet files\n",
        "\n",
        "### `Python`\n"
      ],
      "id": "37a68486"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: python-load-parquet-anes\n",
        "#| eval: false\n",
        "# Specify location of Parquet file\n",
        "parquet_path='./data/2020-anes/anes.parquet'\n",
        "\n",
        "# Pandas\n",
        "    #* Load pandas\n",
        "import pandas as pd\n",
        "    #* Extract the data from parquet file and place it in a Pandas DataFrame\n",
        "anes_df = pd.read_parquet(parquet_path)\n",
        "\n",
        "# Polars\n",
        "    #* Load polars\n",
        "import polars as pl\n",
        "    #* Extract data from parquet file and place it in a Polars DataFrame\n",
        "anes_df = pl.read_parquet(parquet_path)"
      ],
      "id": "python-load-parquet-anes",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: python-profile-load-parquet-anes\n",
        "#| include: false\n",
        "# Specify location of parquet file\n",
        "parquet_path='../data/2020-anes/anes.parquet'\n",
        "\n",
        "# Pandas\n",
        "    #* Define the function to run through\n",
        "def pandas_parquet_extract_anes(path=parquet_path):\n",
        "    anes_df = pd.read_parquet(path)\n",
        "    #* Run the defined function repeatedly and keep track of how long it takes\n",
        "pandas_parquet_anes_time = timeit.repeat(\n",
        "    'pandas_parquet_extract_anes()', # execute the function\n",
        "    'from __main__ import pandas_parquet_extract_anes', # after loading it\n",
        "    number=1, # and run this once\n",
        "    repeat=100 # but run this process 100 times\n",
        ")\n",
        "\n",
        "# Polars\n",
        "    #* Define the function to run through\n",
        "def polars_parquet_extract_anes(path=parquet_path):\n",
        "    anes_dff = pl.read_parquet(path)\n",
        "    #* Run the defined function repeatedly and keep track of how long it takes\n",
        "polars_parquet_anes_time = timeit.repeat(\n",
        "    'polars_parquet_extract_anes()', # execute the function\n",
        "    'from __main__ import polars_parquet_extract_anes', # after loading it\n",
        "    number=1, # and fun this once\n",
        "    repeat=100 # but run this process 100 times\n",
        ")"
      ],
      "id": "python-profile-load-parquet-anes",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "#| echo: false\n",
        "print(\n",
        "    f\"I use the `timeit` package to calculate the average amount of time it took `Pandas` and `Polars` in `Python` to load the parquet file 100 times. It took `Pandas` on average {mean(pandas_parquet_anes_time):.2e} (Std. Deviation = {stdev(pandas_parquet_anes_time):.2e}) seconds and `Polars` an average of {mean(polars_parquet_anes_time):.2e} (Std. Deviation = {stdev(polars_parquet_anes_time):.2e}) seconds.\"\n",
        ")"
      ],
      "id": "07ca7833",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `R`\n",
        "\n",
        "\n",
        "```{r}\n",
        "#| label: r-extract-parquet\n",
        "#| eval: false\n",
        "# Define path to the parquet file\n",
        "parquet_path <- './data/2020-anes/anes.parquet'\n",
        "\n",
        "# Base R\n",
        "    #* Load the arrow package\n",
        "library(arrow)\n",
        "    #* Extract the data from the parquet file and place it in a Tidyverse DataFrame\n",
        "anes_df <- read_arrow(parquet_path)\n",
        "    #* Convert to a Base R DataFrame\n",
        "anes_df <- as.data.frame(anes_df)\n",
        "\n",
        "# Tidyverse\n",
        "    #* Load arrow package\n",
        "library(arrow)\n",
        "    #* Extract data from parquet file\n",
        "anes_df <- read_arrow(parquet_path)\n",
        "\n",
        "# Preview of dataframe\n",
        "head(anes_df)\n",
        "```\n",
        "\n",
        "```{r}\n",
        "#| label: r-profile-extract-parquet\n",
        "#| include: false\n",
        "# Define path to the parquet file\n",
        "parquet_path <- './data/2020-anes/anes.parquet'\n",
        "\n",
        "# Base R\n",
        "    #* Clear tic toc log\n",
        "tic.clearlog()\n",
        "    #* Profiling each step of this process\n",
        "for(i in 1:100){ # repeat the following 100 times\n",
        "    tic(i) # start timer\n",
        "    anes_df <- read_parquet(parquet_path) # load the file\n",
        "    anes_df <- as.data.frame(anes_df) # convert it to a Base R DataFrame\n",
        "    toc(i, quiet=TRUE) # stop timer\n",
        "}\n",
        "base_r_parquet_benchmark_log <- tic.log(format=FALSE) # log how long this loop took\n",
        "base_r_parquet_benchmark <- unlist( # split up the log\n",
        "    lapply( # and for each iteration from the log, do the following\n",
        "        base_r_parquet_benchmark_log\n",
        "        , function(x) x$toc-x$tic # calculate the difference between the start and stop time\n",
        "    )\n",
        ")\n",
        "\n",
        "# Tidyverse\n",
        "    #* Clear tic toc log\n",
        "tic.clearlog()\n",
        "    #* Profiling each step of this process\n",
        "for(i in 1:100) { # repeat the following 100 times\n",
        "    tic(i) # start timer\n",
        "    anes_df <- read_parquet(parquet_path) # load the file\n",
        "    toc(i, quiet=TRUE) # stop timer\n",
        "}\n",
        "tidyverse_parquet_benchmark_log <- tic.log(format=FALSE) # log how long this loop took\n",
        "tidyverse_parquet_benchmark <- unlist( # split up the log\n",
        "    lapply( # and for each iteration from the log, do the following\n",
        "        tidyverse_parquet_benchmark_log\n",
        "        , function(x) x$toc - x$tic # calculate the difference between the start and stop time\n",
        "    )\n",
        ")\n",
        "```\n",
        "\n",
        "```{r}\n",
        "#| output: asis\n",
        "#| echo: false\n",
        "cat(\n",
        "    \"I use the `tictoc` package to calculate the average amount of time it took the `Arrow` package to extract data from the anes.csv file and to generate a `Tidyverse` DataFrame as well as a `Base R` `DataFrame` 100 times. To produce a `Base R` `DataFrame` it took `Arrow` an average of \", format(mean(base_r_parquet_benchmark), digits=3, scientific=TRUE), \" (Std. Deviation = \", format(sd(base_r_parquet_benchmark), digits=3, scientific=TRUE), \") seconds per attempt, while it took `Arrow` an average of \", format(mean(tidyverse_parquet_benchmark), digits=3, scientific=TRUE), \" (Std. Deviation = \", format(sd(tidyverse_parquet_benchmark), digits=3, scientific=TRUE), \") seconds per attempt to produce a `Tidyverse` `DataFrame`.\"\n",
        "    , sep=\"\"\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "### `DuckDB`\n"
      ],
      "id": "2ca9139f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: duckdb-parquet-extract-anes\n",
        "#| eval: false\n",
        "# Load duckdb library\n",
        "import duckdb as db\n",
        "\n",
        "# Connect to database\n",
        "con = db.connect('./data/2020-anes/anes.db')\n",
        "\n",
        "# Create a SQL table from the parquet file\n",
        "con.execute(\n",
        "    '''\n",
        "        CREATE OR REPLACE TABLE -- create or replace a table\n",
        "            main -- ... called main\n",
        "        AS -- with the following data\n",
        "            SELECT -- select the following columns\n",
        "                * -- all of them\n",
        "            FROM -- from\n",
        "                read_parquet('./data/2020-anes/anes.parquet'); -- the anes.parquet file\n",
        "        /*\n",
        "            NOTE:\n",
        "                - I am using the read_parquet function provided by DuckDB\n",
        "                - I am specifying the file inside single quotations within the function.\n",
        "        */\n",
        "    '''\n",
        ")"
      ],
      "id": "duckdb-parquet-extract-anes",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: duckdb-profile-extract-parquet\n",
        "#| include: false\n",
        "# Profile Parquet extraction by duckdb\n",
        "    #* Define function\n",
        "def duckdb_parquet_extract_anes(path=parquet_path):\n",
        "    con.execute(\n",
        "        '''\n",
        "            CREATE OR REPLACE TABLE -- create or replace a table\n",
        "            main -- ... called main\n",
        "            AS -- with the following data \n",
        "                SELECT -- select the following columns\n",
        "                    * -- all of them\n",
        "                FROM -- from\n",
        "                    read_parquet('./data/2020-anes/anes.parquet'); -- the anes.parquet file\n",
        "            /*\n",
        "                NOTE:\n",
        "                    - I am using the read_parquet function provided by DuckDB\n",
        "                    - I am specifying the file inside single quotations within the function.\n",
        "            */\n",
        "        '''\n",
        "    )\n",
        "\n",
        "    #* Profile how long it takes to do it 100 times\n",
        "duckdb_parquet_anes_time = timeit.repeat(\n",
        "    'duckdb_parquet_extract_anes()', # run this function\n",
        "    'from __main__ import duckdb_parquet_extract_anes', # after loading it\n",
        "    number=1, # and run it once\n",
        "    repeat=100 # but run this whole process 100 times\n",
        ")"
      ],
      "id": "duckdb-profile-extract-parquet",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting data from Proprietary file types (i.e., XLSX, DTA, SAV)\n",
        "\n",
        "### XLSX Files\n",
        "\n",
        "#### `Python`\n"
      ],
      "id": "4e3870e2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: python-extracting-xlsx\n",
        "#| eval: false\n",
        "# Define the path to the xlsx file\n",
        "xlsx_path = './data/2020-anes/anes.xlsx'\n",
        "\n",
        "# Pandas\n",
        "    #* Load library\n",
        "import pandas as pd\n",
        "    #* Extract data from the xlsx file\n",
        "anes_df = pd.read_excel(xlsx_path)\n",
        "\n",
        "# Polars\n",
        "    #* Load library\n",
        "import polars as pl\n",
        "    #* Extract data from the xlsx file\n",
        "anes_df = pl.read_excel(xlsx_path)\n",
        "\n",
        "# Display a preview of the data.frame\n",
        "anes_df.head()"
      ],
      "id": "python-extracting-xlsx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: profile-python-extracting-xlsx\n",
        "#| include: false\n",
        "# Define the path to the xlsx file\n",
        "xlsx_path = '../data/2020-anes/anes.xlsx'\n",
        "\n",
        "# Pandas\n",
        "    #* Define function to run through\n",
        "def pandas_extract_xlsx(path=xlsx_path):\n",
        "    anes_df = pd.read_excel(path)\n",
        "\n",
        "    #* Profile how long it takes to run 100 times\n",
        "pandas_extract_xlsx_time = timeit.repeat(\n",
        "    'pandas_extract_xlsx()', # run the function\n",
        "    'from __main__ import pandas_extract_xlsx', # after loading it\n",
        "    number=1, # and run it once\n",
        "    repeat=100 # but run this whole process 100 times\n",
        ")\n",
        "\n",
        "# Polars\n",
        "    #* Define function to run through\n",
        "def polars_extract_xlsx(path=xlsx_path):\n",
        "    anes_df = pl.read_excel(path)\n",
        "\n",
        "    #* Profile how long it takes to run 100 times\n",
        "polars_extract_xlsx_time = timeit.repeat(\n",
        "    'polars_extract_xlsx()', # run the function\n",
        "    'from __main__ import polars_extract_xlsx', # after loading it\n",
        "    number=1, # run this once\n",
        "    repeat=100 # but repeat this whole process 100 times\n",
        ")"
      ],
      "id": "profile-python-extracting-xlsx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "#| echo: false\n",
        "# Print out the descriptive statistics of how muc time it took\n",
        "print(\n",
        "    f\"I use the `timeit` package to calculate the average amount of time it took `Pandas` and `Polars` to extract the file and to generate a `Pandas` and `Polars` `DataFrame` object, respectively. `Pandas` took an average of {mean(pandas_extract_xlsx_time):.2e} (Std. Deviation = {sd(pandas_extract_xlsx_time):.2e}) seconds per attempt, while it took `Polars` an average of {mean(polars_extract_xlsx_time):.2e} (Std. Deviation = {sd(polars_extract_xlsx_time):.2e}) seconds.\"\n",
        ")"
      ],
      "id": "6810430e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### `R`\n",
        "\n",
        "\n",
        "```{r}\n",
        "#| label: r-load-xlsx\n",
        "#| eval: false\n",
        "# Define the path to xlsx file\n",
        "xlsx_path <- './data/2020-anes/anes.xlsx'\n",
        "\n",
        "# Base R\n",
        "    #* Import the readxl package\n",
        "library(readxl)\n",
        "    #* Extract the data and store as a Tidyverse DataFrame\n",
        "anes_df <- read_excel(xlsx_path)\n",
        "    #* Then convert it to a Base R DataFrame\n",
        "anes_df <- as.data.frame(anes_df)\n",
        "\n",
        "# Tidyverse\n",
        "    #* Import package\n",
        "library(readxl)\n",
        "    #* Load file\n",
        "anes_df <- read_excel(xlsx_path)\n",
        "```\n",
        "\n",
        "```{r}\n",
        "#| label: r-profile-extract-parquet\n",
        "#| include: false\n",
        "# Define the path to the xlsx file\n",
        "xlsx_path <- './data/2020-anes/anes.xlsx'\n",
        "\n",
        "# Base R\n",
        "    #* Clear tictoc log\n",
        "tic.clearlog()\n",
        "    #* Profiling each step of this process\n",
        "for(i in 1:100){ # repeat the following 100 times\n",
        "    tic(i) # start timer\n",
        "    anes_df <- read_excel(xlsx_path) # load the file\n",
        "    anes_df <- as.data.frame(anes_df) # convert it to a Base R DataFrame\n",
        "    toc(i, quiet=TRUE) # stop timer\n",
        "}\n",
        "base_r_xlsx_benchmark_log <- tic.log(format=FALSE) # log how long this loop took\n",
        "base_r_xlsx_benchmark <- unlist(# split the log up\n",
        "    lapply(# and do the following to each iteration in the log\n",
        "        base_r_xlsx_benchmark_log\n",
        "        , function(x) x$toc - x$tic # take the difference between the start and stop time\n",
        "    )\n",
        ")\n",
        "\n",
        "# Tidyverse\n",
        "    #* Clear tic toc log\n",
        "tic.clearlog()\n",
        "    #* Profiling each step of this process\n",
        "for(i in 1:100) { # repeat the following 100 times\n",
        "    tic(i) # start timer\n",
        "    anes_df <- read_excel(xlsx_path) # load the file\n",
        "    toc(i, quiet=TRUE) # stop timer\n",
        "}\n",
        "tidyverse_xlsx_benchmark_log <- tic.log(format=FALSE) # log how long this loop took\n",
        "tidyverse_xlsx_benchmark <- unlist( # split the log up\n",
        "    lapply( # and do the following to each iteration in the log\n",
        "        tidyverse_xlsx_benchmark_log\n",
        "        , function(x) x$toc - x$tic # take the difference between the start and stop time\n",
        "    )\n",
        ")\n",
        "```\n",
        "\n",
        "```{r}\n",
        "#| output: asis\n",
        "#| echo: false\n",
        "cat(\n",
        "    \"I use the `tictoc` package to calculate the average amount of time it took the `readxl` package to extract data from the anes.xlsx file and to generate a `Tidyverse` `DataFrame` as well as a `Base R` `DataFrame` 100 times. To produce a `Base R` `DataFrame`, it took `readxl` an average of \", format(mean(base_r_xlsx_benchmark), digits=3, scientific=TRUE), \" (Std. Deviation = \", format(sd(base_r_xlsx_benchmark), digits=3, scientific=TRUE), \") seconds per attempt, while it took `readxl` to produce a `Tidyverse` `DataFrame` an average of \", format(mean(tidyverse_xlsx_benchmark), digits=3, scientific=TRUE), \" (Std. Deviation = \", format(sd(tidyverse_xlsx_benchmark), digits=3, scientific=TRUE), \") seconds per attempt.\"\n",
        "    , sep=\"\"\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "#### `DuckDB`\n"
      ],
      "id": "06f7f973"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: duckdb-extract-xlsx\n",
        "#| eval: false\n",
        "# Import the duckdb library\n",
        "import duckdb as db\n",
        "\n",
        "# Connect to the database with DuckDB's Python API\n",
        "con = db.connect('./data/2020-anes/anes.db')\n",
        "\n",
        "# Need to install and load the Spatial extension to help with reading the excel file\n",
        "con.execute(\n",
        "    '''\n",
        "        INSTALL spatial -- only need to run this once per DuckDB connection.\n",
        "        LOAD spatial -- only need to run this once per DuckDB connection.\n",
        "    '''\n",
        ")\n",
        "\n",
        "# Create a table called main in the database with the extracted data from the xlsx file\n",
        "con.execute(\n",
        "    '''\n",
        "        CREATE OR REPLACE TABLE -- create or replace a table\n",
        "            main -- ... called main\n",
        "        AS \n",
        "            SELECT -- select the following columns\n",
        "                * -- all of them\n",
        "            FROM -- from\n",
        "                st_read('./data/2020-anes/anes.xlsx', layer='Sheet1'); -- the first sheet in the anes.xlsx file\n",
        "        /*\n",
        "            NOTE:\n",
        "              - Using the st_read function provided by the Spatial extension to read the excel file.\n",
        "              - Specify the file inside single quotation marks.\n",
        "              - Need to specify which sheet I should be extracting the data from.\n",
        "        */\n",
        "    '''\n",
        ")"
      ],
      "id": "duckdb-extract-xlsx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: duckdb-profile-extract-xlsx\n",
        "#| include: false\n",
        "# Profile how long it takes to extract data from the xlsx file and generate a table 100 times\n",
        "    #* Define the function to run through\n",
        "def duckdb_extract_xlsx():\n",
        "    con.execute(\n",
        "        '''\n",
        "            CREATE OR REPLACE TABLE -- create or replace a table\n",
        "                main -- ... called main\n",
        "            AS \n",
        "                SELECT -- select the following columns\n",
        "                    * -- all of them\n",
        "                FROM -- from\n",
        "                    st_read('./data/2020-anes/anes.xlsx', layer='Sheet1'); -- the first sheet in the anes.xlsx file\n",
        "            /*\n",
        "                NOTE:\n",
        "                  - Using the st_read function provided by the Spatial extension to read the excel file.\n",
        "                  - Specify the file inside single quotation marks.\n",
        "                  - Need to specify which sheet I should be extracting the data from.\n",
        "            */\n",
        "        '''\n",
        "    )\n",
        "    #* Document how long it takes to run the following 100 times\n",
        "duckdb_extract_xlsx_time = timeit.repeat(\n",
        "    'duckdb_extract_xlsx()', # run the function\n",
        "    'from __main__ import duckdb_extract_xlsx', # after loading the function\n",
        "    number=1, # do this once\n",
        "    repeat=100 # then repeat this process 100 times\n",
        ")"
      ],
      "id": "duckdb-profile-extract-xlsx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "#| echo: false\n",
        "# Print out the descriptive statistics of how much time it took\n",
        "print(\n",
        "    f\"I use the `timeit` package to calculate the average amount of time it took `DuckDB` a table. `DuckDB` took an average of {mean(duckdb_extract_xlsx_time):.2e} (Std. Deviation = {sd(duckdb_extract_xlsx_time):.2e}) seconds per attempt.\"\n",
        ")"
      ],
      "id": "7a2fcef0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DTA Files\n",
        "\n",
        "#### `Python`\n"
      ],
      "id": "04035653"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: python-extract-dta\n",
        "#| eval: false\n",
        "# Define path to dta file\n",
        "dta_path = './data/2020-anes/anes.dta'\n",
        "\n",
        "# Pandas\n",
        "    #* Load pandas library\n",
        "import pandas as pd\n",
        "    #* Extract data from file and place in Pandas DataFrame\n",
        "anes_df = pd.read_stata(dta_path)\n",
        "\n",
        "# Polars\n",
        "    #* Load pandas and polars library\n",
        "import polars as pl\n",
        "import pandas as pd\n",
        "    #* Extract the data from the file and place in Pandas DataFrame\n",
        "anes_df = pd.read_stata(dta_path)\n",
        "    #* Convert Pandas DataFrame to Polars DataFrame\n",
        "anes_df = pl.from_pandas(anes_df)\n",
        "\n",
        "# Preview the DataFrame\n",
        "anes_df.head()"
      ],
      "id": "python-extract-dta",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: python-profile-extract-dta\n",
        "#| include: false\n",
        "# Define path to dta file\n",
        "dta_path = '../data/2020-anes/anes.dta'\n",
        "\n",
        "# Pandas\n",
        "    #* Define function to run through\n",
        "def pandas_extract_dta(path=dta_path):\n",
        "    anes_df = pd.read_stata(dta_path)\n",
        "\n",
        "    #* Profile how long each iteration takes\n",
        "pandas_extract_dta_time = timeit.repeat(\n",
        "    'pandas_extract_dta()', # run this function\n",
        "    'from __main__ import pandas_extract_dta', # after importing the function\n",
        "    number=1, # read it once\n",
        "    repeat=100 # and repeat this whole process 100 times\n",
        ")\n",
        "\n",
        "# Polars\n",
        "    #* Define function to run through\n",
        "def polars_extract_dta(path=dta_path):\n",
        "    anes_df = pd.read_stata(dta_path)\n",
        "    anes_df = pl.from_pandas(anes_df)\n",
        "\n",
        "    #* Profile how long each iteration takes\n",
        "polars_extract_dta_time = timeit.repeat(\n",
        "    'polars_extract_dta()', # run thi function\n",
        "    'from __main__ import polars_extract_dta', # after importing the function\n",
        "    number=1, # read it once\n",
        "    repeat=100 # and repeat the whole process 100 times\n",
        ")"
      ],
      "id": "python-profile-extract-dta",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "#| echo: false\n",
        "# Print out the descriptive statistics of how muc time it took\n",
        "print(\n",
        "    f\"I use the `timeit` package to calculate the average amount of time it took `Pandas` and `Polars` to extract the file and to generate a `Pandas` and `Polars` `DataFrame` object, respectively. `Pandas` took an average of {mean(pandas_extract_dta_time):.2e} (Std. Deviation = {sd(pandas_extract_dta_time):.2e}) seconds per attempt, while it took `Polars` an average of {mean(polars_extract_dta_time):.2e} (Std. Deviation = {sd(polars_extract_dta_time):.2e}) seconds.\"\n",
        ")"
      ],
      "id": "7d260563",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### `R`\n",
        "\n",
        "\n",
        "```{r}\n",
        "#| label: r-extract-dta\n",
        "#| eval: false\n",
        "# Define path to dta file\n",
        "dta_path <- './data/2020-anes/anes.dta'\n",
        "\n",
        "# Base R\n",
        "    #* Load the haven package (foreign package only supports up to STATA 12)\n",
        "library(haven)\n",
        "    #* Extract the dta file and create a Tidyverse DataFrame\n",
        "anes_df <- read_dta(dta_path)\n",
        "    #* Convert the Tidyverse DataFrame into a Base R DataFrame\n",
        "anes_df <- as.data.frame(anes_df) \n",
        "\n",
        "# Tidyverse\n",
        "    #* Load haven package\n",
        "library(haven)\n",
        "    #* Extract the dta file and create a Tidyverse DataFrame\n",
        "anes_df <- read_dta(dta_path)\n",
        "\n",
        "# Preview DataFrame\n",
        "head(anes_df)\n",
        "```\n",
        "\n",
        "```{r}\n",
        "#| label: r-profile-extract-dta\n",
        "#| include: false\n",
        "# Define path to dta file\n",
        "dta_path <- './data/2020-anes/anes.dta'\n",
        "\n",
        "# Base R\n",
        "    #* Clear tictoc log\n",
        "tic.clearlog()\n",
        "    #* Profile how long it takes to load the dta file 100 times\n",
        "for(i in 1:100) {\n",
        "    tic(i) # start the timer at each start of an iteration\n",
        "    anes_df <- read_dta(dta_path) # extract the dta file and place it in a Tidyverse DataFrame\n",
        "    anes_df <- as.data.frame(anes_df) # convert the Tidyverse DataFrame to a Base R DataFrame\n",
        "    toc(i, quiet=TRUE) # stop timer\n",
        "}\n",
        "base_r_dta_benchmark_log <- tic.log(format=FALSE) # log how long this loop took\n",
        "base_r_dta_benchmark <- unlist( # split the log file by iteration\n",
        "    lapply(# for each iteration, do the following\n",
        "    base_r_dta_benchmark_log,\n",
        "    function(x) x$toc - x$tic # calculate the difference between the stop and start time\n",
        "    )\n",
        ")\n",
        "\n",
        "# Tidyverse\n",
        "    #* Clear tictoc log\n",
        "tic.clearlog()\n",
        "    #* Profile how long it takes to load the dta file 100 times.\n",
        "for(i in 1:100) {\n",
        "    tic(i) # start timer at each start of an iteration\n",
        "    anes_df <- read_dta(dta_path) # extract the data from the dta file and place in a DataFrame\n",
        "    toc(i, quiet=TRUE) # stop timer\n",
        "}\n",
        "tidyverse_dta_benchmark_log <- tic.log(format=FALSE) # log how long this loop took\n",
        "tidyverse_dta_benchmark <- unlist(lapply(tidyverse_dta_benchmark_log, function(x) x$toc - x$tic))\n",
        "```\n",
        "\n",
        "```{r}\n",
        "#| output: asis\n",
        "#| echo: false\n",
        "cat(\n",
        "    \"I use the `tictoc` package to calculate the average amount of time it took the `haven` package to extract data from the anes.dta file and to generate a `Tidyverse` `DataFrame` as well as a `Base R` `DataFrame` 100 times. To produce a `Base R` `DataFrame`, it took `haven` an average of \", format(mean(base_r_dta_benchmark), digits=3, scientific=TRUE), \" (Std. Deviation = \", format(sd(base_r_dta_benchmark), digits=3, scientific=TRUE), \") seconds per attempt, while it took `haven` to produce a `Tidyverse` `DataFrame` an average of \", format(mean(tidyverse_dta_benchmark), digits=3, scientific=TRUE), \" (Std. Deviation = \", format(sd(tidyverse_dta_benchmark), digits=3, scientific=TRUE), \") seconds per attempt.\"\n",
        "    , sep=\"\"\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "#### `DuckDB`\n"
      ],
      "id": "d58f6576"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: duckdb-extract-dta\n",
        "#| eval: false\n",
        "# Define path to dta file\n",
        "dta_path = './data/2020-anes/anes.dta'\n",
        "# Load duckdb library\n",
        "import duckdb as db\n",
        "# Connect to database\n",
        "con = db.connect('./data/2020-anes/anes.db')\n",
        "\n",
        "# Load DTA file into Pandas DataFrame\n",
        "    #* Load pandas library\n",
        "import pandas as pd\n",
        "    #* Load the file\n",
        "anes_df = pd.read_stata(dta_path, convert_categoricals=False)\n",
        "\n",
        "# Then create a table object out of the Pandas DataFrame\n",
        "con.execute(\n",
        "    '''\n",
        "        CREATE OR REPLACE TABLE -- create a table\n",
        "            main -- ... called main\n",
        "        AS SELECT -- using the columns\n",
        "            * -- specifically all of them\n",
        "        FROM\n",
        "            anes_df -- from the anes_df pandas DataFrame\n",
        "    '''\n",
        ")"
      ],
      "id": "duckdb-extract-dta",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: duckdb-profile-extract dta\n",
        "#| include: false\n",
        "# Define path to dta file\n",
        "dta_path = '../data/2020-anes/anes.dta'\n",
        "\n",
        "# Define function to run through\n",
        "def duckdb_extract_dta(path=dta_path):\n",
        "    #* Load anes.dta into a Pandas DataFrame\n",
        "    anes_df = pd.read_stata(path, convert_categoricals=False)\n",
        "    #* Then create a table object out of the Pandas DataFrame\n",
        "    con.execute(\n",
        "        '''\n",
        "            CREATE OR REPLACE TABLE -- create a table\n",
        "                main -- ... called main\n",
        "            AS SELECT -- using the columns\n",
        "                * -- specifically all of them\n",
        "            FROM\n",
        "                anes_df -- from the anes_df pandas DataFrame\n",
        "        '''   \n",
        "    )\n",
        "# See how long it takes to go through this function each time 100 times.\n",
        "duckdb_extract_dta_time = timeit.repeat('duckdb_extract_dta()', 'from __main__ import duckdb_extract_dta', number=1, repeat=100)"
      ],
      "id": "duckdb-profile-extract-dta",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "#| echo: false\n",
        "# Print out the descriptive statistics of how muc time it took\n",
        "print(\n",
        "    f\"I use the `timeit` package to calculate the average amount of time it took a combination of `Pandas` and `DuckDB` to extract the file and to generate a table. `Pandas` and `DuckDB` took an average of {mean(duckdb_extract_dta_time):.2e} (Std. Deviation = {sd(duckdb_extract_dta_time):.2e}) seconds per attempt.\"\n",
        ")"
      ],
      "id": "002b05b6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SAV Files\n",
        "\n",
        "#### `Python`\n"
      ],
      "id": "04b9e2f5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: python-extract-sav\n",
        "#| eval: false\n",
        "# Define path of SAV file\n",
        "sav_path = './data/2020-anes/anes.sav'\n",
        "\n",
        "# Pandas\n",
        "    #* Import pandas\n",
        "import pandas as pd\n",
        "    #* Load file\n",
        "anes_df = pd.read_spss(sav_path)\n",
        "\n",
        "# Polars\n",
        "    #* Import pandas and polars\n",
        "import polars as pl\n",
        "import pandas as pd\n",
        "    #* Extract data from the file and store it as a pandas DataFrame\n",
        "anes_df = pd.read_spss(sav_path)\n",
        "    #* Convert pandas DataFrame to polars DataFrame\n",
        "anes_df = pl.from_pandas(anes_df)"
      ],
      "id": "python-extract-sav",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: python-profile-extract-sav\n",
        "#| include: false\n",
        "# Define path of SAV file\n",
        "sav_path = '../data/2020-anes/anes.sav'\n",
        "\n",
        "# Pandas\n",
        "    #* Define function to run through\n",
        "def pandas_extract_sav(path=sav_path):\n",
        "    anes_df = pd.read_spss(path)\n",
        "\n",
        "    #* Profile how long it takes to run through this function 100 times\n",
        "pandas_extract_sav_time = timeit.repeat(\n",
        "    'pandas_extract_sav()', # run the function\n",
        "    'from __main__ import pandas_extract_sav', # after loading it\n",
        "    number=1, # run it one time\n",
        "    repeat=100 # but repeat the whole process 100 times\n",
        ")\n",
        "\n",
        "# Polars\n",
        "    #* Define the function to run through\n",
        "def polars_extract_sav(path=sav_path):\n",
        "    anes_df = pd.read_spss(path)\n",
        "    anes_df = pl.from_pandas(anes_df)\n",
        "\n",
        "    #* Profile how long it takes to run through this function 100 times\n",
        "polars_extract_sav_time = timeit.repeat(\n",
        "    'polars_extract_sav()', # run the function\n",
        "    'from __main__ import polars_extract_sav', # after loading it\n",
        "    number=1, # run it one time\n",
        "    repeat=100 # but repeat the whole process 100 times\n",
        ")"
      ],
      "id": "python-profile-extract-sav",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "#| echo: false\n",
        "# Print out the descriptive statistics of how muc time it took\n",
        "print(\n",
        "    f\"I use the `timeit` package to calculate the average amount of time it took `Pandas` and `Polars` to extract the file and to generate a `Pandas` and `Polars` `DataFrame` object, respectively. `Pandas` took an average of {mean(pandas_extract_sav_time):.2e} (Std. Deviation = {sd(pandas_extract_sav_time):.2e}) seconds per attempt, while it took `Polars` an average of {mean(polars_extract_sav_time):.2e} (Std. Deviation = {sd(polars_extract_sav_time):.2e}) seconds.\"\n",
        ")"
      ],
      "id": "3d707812",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### `R`\n",
        "\n",
        "\n",
        "```{r}\n",
        "#| label: r-load-sav-file\n",
        "#| eval: false\n",
        "# Define file path to SAV file\n",
        "sav_path <- './data/2020-anes/anes.sav'\n",
        "\n",
        "# Base R\n",
        "    #* Load foreign library\n",
        "library(foreign)\n",
        "    #* Extract the data and place it into a Base R DataFrame\n",
        "anes_df <- read.spss(sav_path, to.data.frame=TRUE)\n",
        "\n",
        "# Tidyverse\n",
        "    #* Load haven library\n",
        "library(haven)\n",
        "    #* Extract the data and place it into a Tidyverse DataFrame\n",
        "anes_df <- read_spss(sav_path)\n",
        "```\n",
        "\n",
        "```{r}\n",
        "#| label: r-profile-extract-sav\n",
        "#| include: false\n",
        "# Define file path to SAV file\n",
        "sav_path <- '../data/2020-anes/anes.sav'\n",
        "\n",
        "# Clear tictoc log\n",
        "tic.clearlog()\n",
        "\n",
        "# Base R\n",
        "    # Profile how long it takes to load the sav file 100 times\n",
        "for(i in 1:100) {\n",
        "    tic(i) # start timer\n",
        "    anes_df <- read.spss(sav_path, to.data.frame=TRUE) # extract the data and place it in a DataFrame\n",
        "    toc(i, quiet=TRUE) # stop the timer\n",
        "}\n",
        "base_r_sav_benchmark_log <- tic.log(format=FALSE) # log how long this loop took\n",
        "base_r_sav_benchmark <- unlist( # split the log into the iterations I ran\n",
        "    lapply( # and perform the following on each iteration\n",
        "        base_r_sav_benchmark_log\n",
        "        , function(x) x$toc - x$tic # calculate the difference between the stop and start times\n",
        "    )\n",
        ")\n",
        "\n",
        "# Clear tictoc log\n",
        "tic.clearlog()\n",
        "\n",
        "# Tidyverse\n",
        "    # Profile how long it takes to load the sav file 100 times\n",
        "for(i in 1:100) {\n",
        "    tic(i) # start timer\n",
        "    anes_df <- read.spss(sav_path, to.data.frame=TRUE) # extract the data and place it in a DataFrame\n",
        "    toc(i, quiet=TRUE) # stop the timer\n",
        "}\n",
        "tidyverse_sav_benchmark_log <- tic.log(format=FALSE) # log how long this loop took\n",
        "tidyverse_sav_benchmark <- unlist(# split the log into the iterations I ran\n",
        "    lapply( # and perform the following on each iteration\n",
        "        tidyverse_sav_benchmark_log\n",
        "        , function(x) x$toc - x$tic # calculate the difference between the stop and start times\n",
        "    )\n",
        ")\n",
        "```\n",
        "\n",
        "```{r}\n",
        "#| output: asis\n",
        "#| echo: false\n",
        "cat(\n",
        "    \"I use the `tictoc` package to calculate the average amount of time it took the `foreign` and `haven` packages to extract data from the anes.dta file and to generate a `Base R` and `Tidyverse` `DataFrame` 100 times. To produce a `Base R` `DataFrame`, it took `foreign` an average of \", format(mean(base_r_sav_benchmark), digits=3, scientific=TRUE), \" (Std. Deviation = \", format(sd(base_r_sav_benchmark), digits=3, scientific=TRUE), \") seconds per attempt, while it took `haven` to produce a `Tidyverse` `DataFrame` an average of \", format(mean(tidyverse_sav_benchmark), digits=3, scientific=TRUE), \" (Std. Deviation = \", format(sd(tidyverse_sav_benchmark), digits=3, scientific=TRUE), \") seconds per attempt.\"\n",
        "    , sep=\"\"\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "#### `DuckDB`\n"
      ],
      "id": "2719daec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: duckdb-extract-sav\n",
        "#| eval: false\n",
        "# Define path to sav file\n",
        "sav_path = './data/2020-anes/anes.sav'\n",
        "\n",
        "# Import duckdb library\n",
        "import duckdb as db\n",
        "\n",
        "# Open connection to the database\n",
        "con = db.connect('./data/2020-anes/anes.db')\n",
        "\n",
        "# Extract the data with Pandas and place in a Pandas DataFrame\n",
        "anes_df = pd.read_spss(\n",
        "    sav_path, \n",
        "    convert_categoricals=False # do not try to keep the factor encoding for the columns\n",
        ")\n",
        "\n",
        "# Place the Pandas DataFrame in a table\n",
        "con.execute(\n",
        "    '''\n",
        "        CREATE OR REPLACE TABLE -- create or replace a table\n",
        "            main -- called main\n",
        "        AS -- using the data from the following query\n",
        "            SELECT -- select the following columns\n",
        "                * -- all of them\n",
        "            FROM\n",
        "                anes_df -- from the anes_df Pandas DataFrame\n",
        "    '''\n",
        ")"
      ],
      "id": "duckdb-extract-sav",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: duckdb-profile-extract-sav\n",
        "#| include: false\n",
        "# Define path to sav file\n",
        "sav_path = '../data/2020-anes/anes.sav'\n",
        "\n",
        "# Define function to run through\n",
        "def duckdb_extract_sav(path=sav_path):\n",
        "    #* Extract data from sav file and place in pandas DataFrame\n",
        "    anes_df = pd.read_spss(\n",
        "        path, \n",
        "        convert_categoricals=False # do not try to keep the factor encoding for the columns\n",
        "    )\n",
        "    # Place the Pandas DataFrame in a table\n",
        "    con.execute(\n",
        "        '''\n",
        "            CREATE OR REPLACE TABLE -- create or replace a table\n",
        "                main -- called main\n",
        "            AS -- using the data from the following query\n",
        "                SELECT -- select the following columns\n",
        "                    * -- all of them\n",
        "                FROM\n",
        "                    anes_df -- from the anes_df Pandas DataFrame\n",
        "        '''\n",
        "    )\n",
        "# Run the function 100 times\n",
        "duckdb_extract_sav_time = timeit.repeat(\n",
        "    'duckdb_extract_sav()', # run this function\n",
        "    'from __main__ import duckdb_extract_sav', # after loading the function\n",
        "    number=1, # run this 1 time\n",
        "    repeat=100 # but repeat the whole process 100 times\n",
        ")"
      ],
      "id": "duckdb-profile-extract-sav",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| output: asis\n",
        "#| echo: false\n",
        "# Print out the descriptive statistics of how muc time it took\n",
        "print(\n",
        "    f\"I use the `timeit` package to calculate the average amount of time it took a combination of `Pandas` and `DuckDB` to extract the file and to generate a table. `Pandas` and `DuckDB` took an average of {mean(duckdb_extract_dta_time):.2e} (Std. Deviation = {sd(duckdb_extract_dta_time):.2e}) seconds per attempt.\"\n",
        ")"
      ],
      "id": "e761c7b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing performance across libraries, languages, and file types\n"
      ],
      "id": "1fac1c5f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        " #| label: close-db-connection\n",
        "#| include: false\n",
        "# Close the duckdb connection\n",
        "con.close()"
      ],
      "id": "51f302d1",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}